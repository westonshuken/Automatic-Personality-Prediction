{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "SEED = 53188535"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are three potential datasets for this project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~9k Myers-Briggs Personality Type labeled comments from PersonalityCafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_df = pd.read_csv('data/mbti_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~100k Myers-Briggs Personality Type labeled comments from PersonalityCafe and Google Big Query Reddit users. \n",
    "Posts are preprocessed texts:\n",
    "\n",
    "- No punctuations, stopwords, URLs\n",
    "- Lemmatization\n",
    "- Reconstruct samples to be equal-sized chunks (500 words per sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_df = pd.read_csv('data/MBTI 500.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~1.7M Google Big Query of Reddit comments and their Myers-Briggs Personality Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_df = pd.read_csv('data/mbti_full_pull.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personality Cafe\n",
    "For the baseline, will use the Peronality Cafe Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of all the variables for Myers_Briggs type for future use.\n",
    "pred_lst = ['intp', 'intj', 'entp', 'entj', 'infj', 'infp', 'enfj', \\\n",
    "    'enfp', 'istj', 'isfj', 'estj', 'esfj', 'istp', 'isfp', 'estp', 'esfp']\n",
    "\n",
    "pred_st = set(''.join(pred_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8675 entries, 0 to 8674\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    8675 non-null   object\n",
      " 1   posts   8675 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 135.7+ KB\n"
     ]
    }
   ],
   "source": [
    "cafe_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INFP    0.211182\n",
       "INFJ    0.169452\n",
       "INTP    0.150317\n",
       "INTJ    0.125764\n",
       "ENTP    0.078963\n",
       "ENFP    0.077810\n",
       "ISTP    0.038847\n",
       "ISFP    0.031239\n",
       "ENTJ    0.026628\n",
       "ISTJ    0.023631\n",
       "ENFJ    0.021902\n",
       "ISFJ    0.019135\n",
       "ESTP    0.010259\n",
       "ESFP    0.005533\n",
       "ESFJ    0.004841\n",
       "ESTJ    0.004496\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cafe_df.type.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean = cafe_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['type'] = cafe_clean.type.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['clean_posts'] = cafe_clean.posts.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_remove(post):\n",
    "    return re.sub(r'http\\S+', '', post)\n",
    "\n",
    "def pipe_remove(post):\n",
    "    return re.sub(r'[|]', ' ', post)\n",
    "\n",
    "def punc_remove(post):\n",
    "    return re.sub(r'[\\'_:]', '', post)\n",
    "\n",
    "def remove_dig_token(post):\n",
    "    return [post[i] for i in range(len(post)) if post[i].isalpha()]\n",
    "\n",
    "def remove_stopwords(post):\n",
    "    sw = stopwords.words('english')\n",
    "    return [post[i] for i in range(len(post)) if post[i] not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['clean_posts'] = cafe_clean['clean_posts'].apply(pipe_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['clean_posts'] = cafe_clean['clean_posts'].apply(url_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['clean_posts'] = cafe_clean['clean_posts'].apply(punc_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"(?u)\\b\\w\\w+\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['post_token'] = cafe_clean['clean_posts'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['post_token'] = cafe_clean['post_token'].apply(remove_dig_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['post_token'] = cafe_clean['post_token'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(tokens):\n",
    "    return [lemmatizer.lemmatize(w) for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['post_token'] = cafe_clean['post_token'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(tokens):\n",
    "    long_string = ' '.join(tokens)\n",
    "    return long_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['token_joined'] = cafe_clean['post_token'].apply(join_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['clean_posts'] = cafe_clean['post_token'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8675 entries, 0 to 8674\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   type          8675 non-null   object\n",
      " 1   posts         8675 non-null   object\n",
      " 2   clean_posts   8675 non-null   object\n",
      " 3   post_token    8675 non-null   object\n",
      " 4   token_joined  8675 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 339.0+ KB\n"
     ]
    }
   ],
   "source": [
    "cafe_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>clean_posts</th>\n",
       "      <th>post_token</th>\n",
       "      <th>token_joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>infj</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>enfp intj moment sportscenter top ten play pra...</td>\n",
       "      <td>[enfp, intj, moment, sportscenter, top, ten, p...</td>\n",
       "      <td>enfp intj moment sportscenter top ten play pra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entp</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>im finding lack post alarming sex boring posit...</td>\n",
       "      <td>[im, finding, lack, post, alarming, sex, borin...</td>\n",
       "      <td>im finding lack post alarming sex boring posit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intp</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>good one course say know thats blessing curse ...</td>\n",
       "      <td>[good, one, course, say, know, thats, blessing...</td>\n",
       "      <td>good one course say know thats blessing curse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intj</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>dear intp enjoyed conversation day esoteric ga...</td>\n",
       "      <td>[dear, intp, enjoyed, conversation, day, esote...</td>\n",
       "      <td>dear intp enjoyed conversation day esoteric ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entj</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>youre fired thats another silly misconception ...</td>\n",
       "      <td>[youre, fired, thats, another, silly, misconce...</td>\n",
       "      <td>youre fired thats another silly misconception ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  \\\n",
       "0  infj  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  entp  'I'm finding the lack of me in these posts ver...   \n",
       "2  intp  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  intj  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  entj  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                         clean_posts  \\\n",
       "0  enfp intj moment sportscenter top ten play pra...   \n",
       "1  im finding lack post alarming sex boring posit...   \n",
       "2  good one course say know thats blessing curse ...   \n",
       "3  dear intp enjoyed conversation day esoteric ga...   \n",
       "4  youre fired thats another silly misconception ...   \n",
       "\n",
       "                                          post_token  \\\n",
       "0  [enfp, intj, moment, sportscenter, top, ten, p...   \n",
       "1  [im, finding, lack, post, alarming, sex, borin...   \n",
       "2  [good, one, course, say, know, thats, blessing...   \n",
       "3  [dear, intp, enjoyed, conversation, day, esote...   \n",
       "4  [youre, fired, thats, another, silly, misconce...   \n",
       "\n",
       "                                        token_joined  \n",
       "0  enfp intj moment sportscenter top ten play pra...  \n",
       "1  im finding lack post alarming sex boring posit...  \n",
       "2  good one course say know thats blessing curse ...  \n",
       "3  dear intp enjoyed conversation day esoteric ga...  \n",
       "4  youre fired thats another silly misconception ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cafe_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding binary targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['i/e'] = cafe_clean['type'].map(lambda x: x[0])\n",
    "cafe_clean['n/s'] = cafe_clean['type'].map(lambda x: x[1])\n",
    "cafe_clean['t/f'] = cafe_clean['type'].map(lambda x: x[2])\n",
    "cafe_clean['p/j'] = cafe_clean['type'].map(lambda x: x[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i    6676\n",
      "e    1999\n",
      "Name: i/e, dtype: int64\n",
      "n    7478\n",
      "s    1197\n",
      "Name: n/s, dtype: int64\n",
      "f    4694\n",
      "t    3981\n",
      "Name: t/f, dtype: int64\n",
      "p    5241\n",
      "j    3434\n",
      "Name: p/j, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(cafe_clean['i/e'].value_counts())\n",
    "print(cafe_clean['n/s'].value_counts())\n",
    "print(cafe_clean['t/f'].value_counts())\n",
    "print(cafe_clean['p/j'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>clean_posts</th>\n",
       "      <th>post_token</th>\n",
       "      <th>token_joined</th>\n",
       "      <th>i/e</th>\n",
       "      <th>n/s</th>\n",
       "      <th>t/f</th>\n",
       "      <th>p/j</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>infj</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>enfp intj moment sportscenter top ten play pra...</td>\n",
       "      <td>[enfp, intj, moment, sportscenter, top, ten, p...</td>\n",
       "      <td>enfp intj moment sportscenter top ten play pra...</td>\n",
       "      <td>i</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entp</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>im finding lack post alarming sex boring posit...</td>\n",
       "      <td>[im, finding, lack, post, alarming, sex, borin...</td>\n",
       "      <td>im finding lack post alarming sex boring posit...</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intp</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>good one course say know thats blessing curse ...</td>\n",
       "      <td>[good, one, course, say, know, thats, blessing...</td>\n",
       "      <td>good one course say know thats blessing curse ...</td>\n",
       "      <td>i</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intj</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>dear intp enjoyed conversation day esoteric ga...</td>\n",
       "      <td>[dear, intp, enjoyed, conversation, day, esote...</td>\n",
       "      <td>dear intp enjoyed conversation day esoteric ga...</td>\n",
       "      <td>i</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entj</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>youre fired thats another silly misconception ...</td>\n",
       "      <td>[youre, fired, thats, another, silly, misconce...</td>\n",
       "      <td>youre fired thats another silly misconception ...</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>isfp</td>\n",
       "      <td>'https://www.youtube.com/watch?v=t8edHB_h908||...</td>\n",
       "      <td>ixfp always think cat fi doms reason especiall...</td>\n",
       "      <td>[ixfp, always, think, cat, fi, doms, reason, e...</td>\n",
       "      <td>ixfp always think cat fi doms reason especiall...</td>\n",
       "      <td>i</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>enfp</td>\n",
       "      <td>'So...if this thread already exists someplace ...</td>\n",
       "      <td>thread already exists someplace else heck dele...</td>\n",
       "      <td>[thread, already, exists, someplace, else, hec...</td>\n",
       "      <td>thread already exists someplace else heck dele...</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>intp</td>\n",
       "      <td>'So many questions when i do these things.  I ...</td>\n",
       "      <td>many question thing would take purple pill pic...</td>\n",
       "      <td>[many, question, thing, would, take, purple, p...</td>\n",
       "      <td>many question thing would take purple pill pic...</td>\n",
       "      <td>i</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>infp</td>\n",
       "      <td>'I am very conflicted right now when it comes ...</td>\n",
       "      <td>conflicted right come wanting child honestly m...</td>\n",
       "      <td>[conflicted, right, come, wanting, child, hone...</td>\n",
       "      <td>conflicted right come wanting child honestly m...</td>\n",
       "      <td>i</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>infp</td>\n",
       "      <td>'It has been too long since I have been on per...</td>\n",
       "      <td>long since personalitycafe although doesnt see...</td>\n",
       "      <td>[long, since, personalitycafe, although, doesn...</td>\n",
       "      <td>long since personalitycafe although doesnt see...</td>\n",
       "      <td>i</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts  \\\n",
       "0     infj  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1     entp  'I'm finding the lack of me in these posts ver...   \n",
       "2     intp  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3     intj  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4     entj  'You're fired.|||That's another silly misconce...   \n",
       "...    ...                                                ...   \n",
       "8670  isfp  'https://www.youtube.com/watch?v=t8edHB_h908||...   \n",
       "8671  enfp  'So...if this thread already exists someplace ...   \n",
       "8672  intp  'So many questions when i do these things.  I ...   \n",
       "8673  infp  'I am very conflicted right now when it comes ...   \n",
       "8674  infp  'It has been too long since I have been on per...   \n",
       "\n",
       "                                            clean_posts  \\\n",
       "0     enfp intj moment sportscenter top ten play pra...   \n",
       "1     im finding lack post alarming sex boring posit...   \n",
       "2     good one course say know thats blessing curse ...   \n",
       "3     dear intp enjoyed conversation day esoteric ga...   \n",
       "4     youre fired thats another silly misconception ...   \n",
       "...                                                 ...   \n",
       "8670  ixfp always think cat fi doms reason especiall...   \n",
       "8671  thread already exists someplace else heck dele...   \n",
       "8672  many question thing would take purple pill pic...   \n",
       "8673  conflicted right come wanting child honestly m...   \n",
       "8674  long since personalitycafe although doesnt see...   \n",
       "\n",
       "                                             post_token  \\\n",
       "0     [enfp, intj, moment, sportscenter, top, ten, p...   \n",
       "1     [im, finding, lack, post, alarming, sex, borin...   \n",
       "2     [good, one, course, say, know, thats, blessing...   \n",
       "3     [dear, intp, enjoyed, conversation, day, esote...   \n",
       "4     [youre, fired, thats, another, silly, misconce...   \n",
       "...                                                 ...   \n",
       "8670  [ixfp, always, think, cat, fi, doms, reason, e...   \n",
       "8671  [thread, already, exists, someplace, else, hec...   \n",
       "8672  [many, question, thing, would, take, purple, p...   \n",
       "8673  [conflicted, right, come, wanting, child, hone...   \n",
       "8674  [long, since, personalitycafe, although, doesn...   \n",
       "\n",
       "                                           token_joined i/e n/s t/f p/j  \n",
       "0     enfp intj moment sportscenter top ten play pra...   i   n   f   j  \n",
       "1     im finding lack post alarming sex boring posit...   e   n   t   p  \n",
       "2     good one course say know thats blessing curse ...   i   n   t   p  \n",
       "3     dear intp enjoyed conversation day esoteric ga...   i   n   t   j  \n",
       "4     youre fired thats another silly misconception ...   e   n   t   j  \n",
       "...                                                 ...  ..  ..  ..  ..  \n",
       "8670  ixfp always think cat fi doms reason especiall...   i   s   f   p  \n",
       "8671  thread already exists someplace else heck dele...   e   n   f   p  \n",
       "8672  many question thing would take purple pill pic...   i   n   t   p  \n",
       "8673  conflicted right come wanting child honestly m...   i   n   f   p  \n",
       "8674  long since personalitycafe although doesnt see...   i   n   f   p  \n",
       "\n",
       "[8675 rows x 9 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cafe_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cafe_clean['token_joined']\n",
    "y = cafe_clean['type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vc = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count = count_vc.fit_transform(X_train)\n",
    "X_test_count = count_vc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV: 0.6156, A: 0.5984324573536192\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier(class_weight='balanced', random_state=SEED)\n",
    "\n",
    "sgd.fit(X_train_count, y_train)\n",
    "\n",
    "cv_score = cross_val_score(sgd, X_train_count, y_train, cv=5)\n",
    "cv_score_mean = round(np.mean(cv_score), 4)\n",
    "\n",
    "y_pred = sgd.predict(X_test_count)\n",
    "acc_score = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print(f\"CV: {cv_score_mean}, A: {acc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "plot_confusion_matrix(sgd, X_test_count, y_test, ax=ax, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE\n",
    "# ---------\n",
    "# All classes ~60%\n",
    "# i/e ~85%\n",
    "# n/s ~90%\n",
    "# p/j ~77%\n",
    "# t/f ~85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understample test set for binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(cafe_clean, random_state=SEED)\n",
    "\n",
    "X_train = train_set['token_joined']\n",
    "X_train = np.array(X_train).reshape(-1, 1)\n",
    "\n",
    "y_train = train_set['t/f']\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# instantiating the random undersampler\n",
    "rus = RandomUnderSampler() \n",
    "\n",
    "# resampling training set X & y\n",
    "X_rus, y_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# new class distribution\n",
    "print(np.unique(y_train, return_counts=True))\n",
    "print(np.unique(y_rus, return_counts=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rus.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rus = pd.Series(X_rus.reshape(-1))\n",
    "y_rus = pd.Series(y_rus.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count = count_vc.fit_transform(X_rus)\n",
    "X_test_count = count_vc.transform(test_set['token_joined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(class_weight='balanced', random_state=SEED)\n",
    "\n",
    "sgd.fit(X_train_count, y_rus)\n",
    "\n",
    "cv_score = cross_val_score(sgd, X_train_count, y_rus, cv=5)\n",
    "cv_score_mean = round(np.mean(cv_score), 4)\n",
    "\n",
    "y_pred = sgd.predict(X_test_count)\n",
    "acc_score = accuracy_score(y_pred, test_set['t/f'])\n",
    "\n",
    "print(f\"CV: {cv_score_mean}, A: {acc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "plot_confusion_matrix(sgd, X_test_count, test_set['t/f'], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling\n",
    "# ---------\n",
    "# i/e ~81%\n",
    "# n/s ~83%\n",
    "# p/j ~76%\n",
    "# t/f ~83%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both DF\n",
    "\n",
    "Comparing baseline models to this other dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_clean = both_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_clean['type'] = both_clean['type'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_clean['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_clean['i/e'] = both_clean['type'].map(lambda x: x[0])\n",
    "both_clean['n/s'] = both_clean['type'].map(lambda x: x[1])\n",
    "both_clean['t/f'] = both_clean['type'].map(lambda x: x[2])\n",
    "both_clean['p/j'] = both_clean['type'].map(lambda x: x[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(both_clean['i/e'].value_counts())\n",
    "print(both_clean['n/s'].value_counts())\n",
    "print(both_clean['t/f'].value_counts())\n",
    "print(both_clean['p/j'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = both_clean['posts']\n",
    "y = both_clean['type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vc = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count = count_vc.fit_transform(X_train)\n",
    "X_test_count = count_vc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(class_weight='balanced', random_state=SEED)\n",
    "\n",
    "sgd.fit(X_train_count, y_train)\n",
    "\n",
    "cv_score = cross_val_score(sgd, X_train_count, y_train, cv=5)\n",
    "cv_score_mean = round(np.mean(cv_score), 4)\n",
    "\n",
    "y_pred = sgd.predict(X_test_count)\n",
    "acc_score = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print(f\"CV: {cv_score_mean}, A: {round(acc_score, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.rc('xtick', labelsize=22) \n",
    "plt.rc('ytick', labelsize=22) \n",
    "plot_confusion_matrix(sgd, X_test_count, y_test, ax=ax, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE (100k set)\n",
    "# ---------\n",
    "# All ~82%\n",
    "# i/e ~85%\n",
    "# n/s ~96%\n",
    "# p/j ~86%\n",
    "# t/f ~92%\n",
    "\n",
    "# BASELINE (Cafe 9k set)\n",
    "# ---------\n",
    "# All classes ~60%\n",
    "# i/e ~85%\n",
    "# n/s ~90%\n",
    "# p/j ~77%\n",
    "# t/f ~85%\n",
    "\n",
    "# Undersampling (Cafe)\n",
    "# ---------\n",
    "# i/e ~81%\n",
    "# n/s ~83%\n",
    "# p/j ~76%\n",
    "# t/f ~83%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean = gbq_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean['author_flair_text'] = gbq_clean['author_flair_text'].str.lower()\n",
    "gbq_clean['subreddit'] = gbq_clean['subreddit'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist\n",
    "regex_mbti = '|'.join([\"(\" + i + \")\" for i in whitelist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_mbti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mbti in whitelist:\n",
    "    gbq_clean['author_flair_text'] = gbq_clean['author_flair_text'].mask(gbq_clean['author_flair_text'].str.match(\"(?:\" + mbti + \")\"), mbti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbq_clean['author_flair_text'] = gbq_clean['author_flair_text'].mask(gbq_clean['author_flair_text'].str.match(r\"\\b-infj\", case=False), 'infj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean = gbq_clean[gbq_clean['author_flair_text'].str.match(regex_mbti)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean['author_flair_text'].value_counts().to_frame('counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean['i/e'] = gbq_clean['author_flair_text'].map(lambda x: x[0])\n",
    "gbq_clean['n/s'] = gbq_clean['author_flair_text'].map(lambda x: x[1])\n",
    "gbq_clean['t/f'] = gbq_clean['author_flair_text'].map(lambda x: x[2])\n",
    "gbq_clean['p/j'] = gbq_clean['author_flair_text'].map(lambda x: x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean.columns[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in gbq_clean.columns[-4:]:\n",
    "    plt.bar(gbq_clean[column].value_counts(normalize=True).index, gbq_clean[column].value_counts(normalize=True).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_clean import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean = gbq_clean.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean['body'] = gbq_clean['body'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean['clean_posts'] = gbq_clean['body'].apply(pipe_remove)\n",
    "gbq_clean['clean_posts'] = gbq_clean['body'].apply(url_remove)\n",
    "gbq_clean['clean_posts'] = gbq_clean['body'].apply(punc_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"(?u)\\b\\w\\w+\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean['post_token'] = gbq_clean['clean_posts'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean['post_token'] = gbq_clean['post_token'].apply(remove_dig_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean['post_token'] = gbq_clean['post_token'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean['post_token'] = gbq_clean['post_token'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean = gbq_clean[gbq_clean['post_token'].apply(lambda x: len(x) > 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean['token_joined'] = gbq_clean['post_token'].apply(join_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_clean.to_pickle(\"./clean_df.pkl\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling = pd.read_pickle('./clean_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling = df_modeling.rename(columns={'author_flair_text': 'type', 'body': 'posts'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling = pd.concat([df_modeling, cafe_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling['length'] = df_modeling['post_token'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling_two = df_modeling[df_modeling['post_token'].apply(lambda x: len(x) > 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling_two.to_pickle(\"./finalmodeling_df.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling_two['type'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_modeling_two['token_joined']\n",
    "y = df_modeling_two['type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vc = CountVectorizer(ngram_range=(1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vc = TfidfVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count = tfidf_vc.fit_transform(X_train)\n",
    "X_test_count = tfidf_vc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vc.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(random_state=SEED)\n",
    "\n",
    "sgd.fit(X_train_count, y_train)\n",
    "\n",
    "cv_score = cross_val_score(sgd, X_train_count, y_train, cv=5)\n",
    "cv_score_mean = round(np.mean(cv_score), 4)\n",
    "\n",
    "y_pred = sgd.predict(X_test_count)\n",
    "acc_score = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print(f\"CV: {cv_score_mean}, A: {round(acc_score, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(sgd, open('baseline_sgd.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.rc('xtick', labelsize=22) \n",
    "plt.rc('ytick', labelsize=22) \n",
    "plot_confusion_matrix(sgd, X_test_count, y_test, ax=ax, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "033c84df5fb4c613acf884834f63930b25da6784759ce0fb831a430fcd673895"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('learn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
