{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import pos_tag\n",
    "\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from tqdm import tqdm\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from mbti import run_models, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/cafe_clean_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True)\n",
    "test_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_gens(dataframe, tokens_only=False):\n",
    "    for i, post in enumerate(dataframe['joined_tokens']):\n",
    "        tokens = gensim.utils.simple_preprocess(post)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, dataframe.loc[i, 't/f'])\n",
    "\n",
    "train_corpus = list(tokenize_gens(train_df))\n",
    "test_corpus = list(tokenize_gens(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['last', 'christmas', 'celebrate', 'entire', 'universe', 'ever', 'movie', 'vendetta', 'valeries', 'letter', 'silver', 'lining', 'playbook', 'dinner', 'eggplant', 'scene', 'rain', 'send', 'lenovo', 'use', 'tapatalk', 'everything', 'make', 'perfect', 'sense', 'head', 'reality', 'doesnt', 'seem', 'catch', 'sent', 'lenovo', 'use', 'tapatalk', 'radiohead', 'placebo', 'depeche', 'mode', 'queen', 'stone', 'age', 'muse', 'eel', 'nick', 'cave', 'nirvana', 'sent', 'lenovo', 'use', 'tapatalk', 'send', 'lenovo', 'use', 'tapatalk', 'well', 'starter', 'free', 'love', 'personally', 'nothing', 'pardon', 'french', 'fuck', 'around', 'open', 'relationship', 'free', 'love', 'right', 'apology', 'enigmatic', 'case', 'whole', 'point', 'get', 'notice', 'let', 'alone', 'someone', 'propose', 'ever', 'even', 'person', 'would', 'soulmate', 'usually', 'conventional', 'way', 'believe', 'intention', 'take', 'consideration', 'seem', 'manipulative', 'surface', 'especially', 'try', 'help', 'someone', 'break', 'mask', 'persona', 'make', 'list', 'top', 'head', 'moment', 'body', 'cage', 'arcade', 'fire', 'people', 'help', 'people', 'birdy', 'prefer', 'version', 'original', 'rare', 'kryptonite', 'door', 'face', 'opponent', 'obviously', 'im', 'pretty', 'good', 'poker', 'play', 'fun', 'much', 'gambler', 'want', 'think', 'could', 'pull', 'sent', 'textbook', 'dear', 'love', 'deeply', 'loyal', 'devote', 'friend', 'sometimes', 'need', 'ask', 'know', 'never', 'get', 'straight', 'answer', 'question', 'since', 'alright', 'here', 'evey', 'reborn', 'vendetta', 'sicilian', 'spawn', 'nigger', 'true', 'romance', 'well', 'hi', 'im', 'im', 'slowly', 'try', 'determine', 'enneagram', 'type', 'seem', 'lean', 'towards', 'comment', 'opinion', 'indicate', 'opposite', 'welcome', 'sent', 'lenovo', 'use', 'tapatalk', 'type', 'fictional', 'character', 'resonate', 'past', 'present', 'evey', 'hammond', 'vendetta', 'vanessa', 'ives', 'penny', 'dreadful', 'belle', 'beauty', 'beast', 'plenty', 'opportunity', 'extremely', 'picky', 'sent', 'lenovo', 'use', 'tapatalk', 'vendetta', 'fight', 'club', 'fountain', 'requiem', 'dream', 'interview', 'vampire', 'hi', 'sophi', 'thanks', 'reply', 'much', 'appreciate', 'feel', 'somehow', 'want', 'help', 'connect', 'sound', 'like', 'genuinely', 'care', 'dont', 'know', 'im', 'hi', 'great', 'feedback', 'recognizable', 'thanks', 'notice', 'clear', 'communication', 'prove', 'practical', 'application', 'deal', 'emotion', 'work', 'best', 'life', 'end', 'right', 'wouldnt', 'time', 'answer', 'question', 'would', 'sake', 'wouldnt', 'change', 'either', 'silence', 'would', 'record', 'know', 'lot', 'different', 'also', 'cognitively', 'much', 'friend', 'boyfriend', 'even', 'old', 'teacher', 'mine', 'one', 'thing', 'people', 'see', 'hi', 'everyone', 'please', 'allow', 'briefly', 'introduce', 'mbti', 'speak', 'pretty', 'much', 'grasp', 'big', 'part', 'cognitive', 'function', 'gut', 'always', 'right', 'seldom', 'listen', 'want', 'wrong', 'time', 'might', 'like', 'analyse', 'front', 'others', 'even', 'though', 'deserve', 'verbal', 'slap', 'face', 'keep', 'mouth', 'shut', 'hypocrite', 'euh', 'read', 'forum', 'thread', 'observe', 'must', 'read', 'reserve', 'express', 'true', 'self', 'also', 'ask', 'specific', 'target', 'question', 'might', 'better', 'approach', 'cause', 'feel', 'like', 'dance', 'leader', 'way', 'say', 'yes', 'one', 'without', 'doubt', 'dont', 'believe', 'thing', 'best', 'movie', 'ever', 'subjective', 'one', 'see', 'every', 'rootunder', 'active', 'sacralunder', 'active', 'navelunder', 'active', 'heartopen', 'throatopen', 'third', 'eyeover', 'active', 'crownopen', 'masterpiece', 'describe', 'best', 'cant', 'really', 'remember', 'ever', 'feel', 'like', 'one', 'thats', 'get', 'smart', 'as', 'figure', 'santa', 'isnt', 'real', 'seriously', 'though', 'didnt', 'even', 'feel', 'connect', 'hi', 'pseudolonewolf', 'study', 'philosophy', 'one', 'course', 'could', 'combine', 'full', 'time', 'job', 'genuinly', 'interested', 'would', 'free', 'open', 'choice'], tags='f'), TaggedDocument(words=['genderfemale', 'hair', 'color', 'dark', 'brown', 'part', 'dye', 'burgundy', 'red', 'usually', 'fade', 'hair', 'style', 'layer', 'split', 'middle', 'eye', 'shapebig', 'doll', 'eye', 'eye', 'colorbrown', 'aspect', 'personality', 'make', 'unsure', 'type', 'people', 'see', 'think', 'yearn', 'life', 'knowledge', 'comfort', 'independence', 'understanding', 'heart', 'swirlyends', 'skillchaos', 'nonrouted', 'guess', 'call', 'love', 'sharp', 'nife', 'youre', 'nite', 'shin', 'armor', 'nope', 'okay', 'aspect', 'personality', 'make', 'unsure', 'type', 'fact', 'would', 'like', 'intelligent', 'subject', 'wayy', 'creative', 'thinking', 'process', 'never', 'mean', 'cold', 'never', 'really', 'want', 'see', 'screwed', 'side', 'keep', 'locked', 'inside', 'deep', 'always', 'seem', 'get', 'never', 'really', 'want', 'go', 'chose', 'dem', 'tho', 'xp', 'nice', 'sarcasm', 'use', 'body', 'language', 'talk', 'something', 'interesting', 'dont', 'try', 'act', 'smart', 'also', 'giggle', 'lot', 'earth', 'think', 'think', 'theyre', 'funny', 'enjoy', 'time', 'together', 'use', 'mbti', 'seem', 'like', 'think', 'pretty', 'cool', 'would', 'perfect', 'soceity', 'creative', 'thinking', 'pssh', 'rica', 'por', 'que', 'wayne', 'invita', 'tu', 'fiesta', 'de', 'adios', 'id', 'choose', 'red', 'second', 'choice', 'grey', 'id', 'iago', 'othello', 'amon', 'legend', 'korra', 'technically', 'cant', 'even', 'since', 'youre', 'make', 'observe', 'environment', 'grow', 'adjust', 'adapt', 'way', 'make', 'part', 'everyone', 'life', 'tell', 'next', 'thing', 'know', 'criminal', 'record', 'think', 'belief', 'knowledge', 'dont', 'believe', 'god', 'doesnt', 'make', 'sense', 'everything', 'possible', 'concept', 'god', 'probably', 'existence', 'god', 'scientifically', 'agoraphobia', 'couldnt', 'go', 'school', 'even', 'store', 'without', 'literally', 'panic', 'attack', 'go', 'specialist', 'give', 'lexapro', 'feel', 'well', 'still', 'dont', 'think', 'oh', 'okay', 'thanks', 'even', 'much', 'like', 'read', 'novel', 'shouldnt', 'problem', 'thanks', 'guy', 'really', 'help', 'clear', 'confusion', 'sarcasm', 'uhm', 'okay', 'think', 'ti', 'perceive', 'function', 'think', 'bit', 'unorganized', 'come', 'seem', 'organized', 'person', 'head', 'te', 'probably', 'like', 'think', 'loud', 'okay', 'idea', 'explain', 'seem', 'like', 'te', 'somewhere', 'get', 'app', 'call', 'oh', 'cube', 'iphone', 'solve', 'around', 'min', 'dont', 'really', 'go', 'time', 'unless', 'someone', 'ask', 'solve', 'seem', 'like', 'te', 'everything', 'explain', 'think', 'come', 'organize', 'lee', 'ask', 'open', 'idea', 'change', 'uh', 'idk', 'think', 'like', 'look', 'express', 'like', 'sound', 'like', 'intx', 'sound', 'like', 'introverted', 'extrovert', 'fi', 'plus', 'seem', 'like', 'might', 'also', 'develop', 'si', 'anything', 'may', 'affect', 'way', 'answer', 'question', 'example', 'stressful', 'time', 'mental', 'illness', 'medication', 'special', 'life', 'circumstance', 'useful', 'information', 'include', 'idk', 'personally', 'cant', 'stand', 'dad', 'youre', 'get', 'upset', 'nothing', 'im', 'upset', 'id', 'get', 'head', 'ache', 'feel', 'instructor', 'feel', 'project', 'uhmmm', 'wut', 'dont', 'know', 'yep', 'sister', 'exactly', 'like', 'think', 'anger', 'issue', 'shes', 'asocial', 'ruin', 'fun', 'shes', 'always', 'chat', 'online', 'friend', 'nice', 'hurt', 'cuhs', 'shes', 'think', 'much', 'end', 'get', 'red', 'patch', 'skin', 'hurt', 'internally', 'like', 'someone', 'suck', 'blood', 'get', 'checked', 'grade', 'doctor', 'say', 'stress', 'anxiety', 'remember', 'elementary', 'use', 'get', 'detention', 'card', 'thingys', 'well', 'high', 'school', 'class', 'use', 'system', 'think', 'less', 'successful', 'well', 'think', 'modern', 'society', 'want', 'live', 'sj', 'lifestyle', 'thats', 'dont', 'agree', 'im', 'except', 'im', 'still', 'high', 'school', 'realise', 'earlier', 'really', 'want', 'would', 'ask', 'two', 'true', 'god', 'would', 'answer', 'question', 'true', 'find', 'one', 'true', 'false', 'ask', 'ja', 'mean', 'yes', 'aand', 'idk', 'might', 'work', 'yeah', 'well', 'kinda', 'whenever', 'im', 'class', 'school', 'dont', 'feel', 'good', 'always', 'try', 'go', 'bathroom', 'put', 'head', 'take', 'care', 'everyone', 'keep', 'want', 'go', 'chelow', 'kabab', 'wikipedia', 'free', 'encyclopedia', 'tryna', 'explain', 'meat', 'like', 'eat', 'someone', 'xd', 'maybe', 'cuhs', 'sister', 'one', 'like', 'fi', 'ni', 'mixed', 'bad', 'way', 'one', 'know', 'stuck', 'little', 'fantasy', 'world', 'cant', 'really', 'deal', 'reality', 'go', 'time', 'sonic', 'gifs', 'comeback', 'argument', 'performance', 'ive', 'thinking', 'act', 'like', 'extroverted', 'type', 'socialize', 'example', 'would', 'act', 'like', 'socialize', 'since', 'ne', 'fe', 'active', 'time', 'guy', 'totally', 'miss', 'cosmo', 'never', 'really', 'liked', 'shouldnt', 'leave', 'seem', 'like', 'maybe', 'probably', 'type', 'red', 'hot', 'bloody', 'dark', 'cat', 'cute', 'evil', 'independent', 'room', 'comfortable', 'fit', 'alone', 'id', 'confused', 'try', 'figure', 'thing', 'thats', 'interest', 'like', 'dance', 'perform'], tags='t')]\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['ive', 'get', 'skeleton', 'pun', 'mastoid', 'actually', 'theyre', 'little', 'hard', 'come', 'might', 'fib', 'ula', 'xiphoid', 'look', 'today', 'nothing', 'wrong', 'long', 'process', 'today', 'discover', 'favorite', 'garden', 'free', 'student', 'great', 'theyre', 'perfect', 'relax', 'there', 'quiet', 'place', 'full', 'swing', 'call', 'cathedral', 'sittapygmaea', 'thanks', 'ill', 'start', 'small', 'sound', 'good', 'also', 'may', 'mention', 'really', 'like', 'avatar', 'username', 'ive', 'miss', 'see', 'nutchatches', 'lot', 'since', 'move', 'wish', 'could', 'set', 'boundary', 'friend', 'like', 'big', 'pet', 'peeve', 'dont', 'like', 'call', 'stupid', 'even', 'joke', 'way', 'feel', 'like', 'id', 'get', 'call', 'want', 'post', 'regularly', 'keep', 'delete', 'get', 'frustrate', 'every', 'time', 'come', 'back', 'havent', 'write', 'lately', 'either', 'think', 'expression', 'form', 'know', 'suedeswede', 'thought', 'exactly', 'gender', 'feel', 'like', 'weird', 'ritual', 'sometimes', 'wish', 'casual', 'part', 'reason', 'post', 'first', 'place', 'customer', 'id', 'help', 'earlier', 'want', 'size', 'galaxy', 'eat', 'star', 'gas', 'giant', 'without', 'notice', 'get', 'upset', 'bad', 'isnt', 'love', 'look', 'like', 'isnt', 'gender', 'role', 'even', 'thing', 'look', 'great', 'youre', 'comfortable', 'traditionally', 'masculine', 'feminine', 'dont', 'cut', 'people', 'dont', 'fit', 'expectation', 'base', 'weird', 'guess', 'get', 'know', 'people', 'like', 'read', 'book', 'take', 'time', 'may', 'understand', 'certain', 'part', 'know', 'pretty', 'well', 'like', 'youve', 'finish', 'book', 'youll', 'always', 'past', 'go', 'cannot', 'harm', 'anymore', 'future', 'fast', 'come', 'always', 'flinch', 'first', 'settle', 'gentle', 'present', 'cope', 'mostly', 'get', 'compliment', 'criticism', 'throw', 'completely', 'guard', 'must', 'self', 'aware', 'anyway', 'thing', 'good', 'lately', 'dont', 'feel', 'control', 'anymore', 'kind', 'city', 'people', 'write', 'weird', 'haikus', 'highway', 'bridge', 'anyway', 'keep', 'walk', 'past', 'bridge', 'brush', 'creek', 'there', 'part', 'one', 'tell', 'bluebonnet', 'smell', 'like', 'honey', 'somehow', 'warm', 'individual', 'flower', 'scent', 'big', 'field', 'even', 'pretty', 'mild', 'wholesome', 'thinking', 'think', 'might', 'try', 'ask', 'really', 'close', 'friend', 'mine', 'realize', 'picture', 'anyone', 'else', 'didnt', 'feel', 'right', 'maybe', 'shouldnt', 'dont', 'want', 'make', 'thing', 'find', 'rope', 'swing', 'hang', 'little', 'waterfall', 'today', 'try', 'even', 'though', 'water', 'freeze', 'nice', 'though', 'super', 'warm', 'rainy', 'awhile', 'imagine', 'old', 'self', 'would', 'wop', 'head', 'cane', 'say', 'something', 'along', 'line', 'stop', 'worry', 'much', 'spend', 'time', 'ocean', 'tell', 'people', 'love', 'attic', 'recipe', 'sound', 'gorgeous', 'plus', 'super', 'rainy', 'soup', 'sound', 'wonderful', 'let', 'know', 'turn', 'sage', 'del', 'viento', 'mine', 'haha', 'join', 'ive', 'grown', 'awful', 'beard', 'dont', 'know', 'exactly', 'good', 'mix', 'thing', 'sometimes', 'even', 'well', 'maybe', 'isnt', 'awful', 'part', 'try', 'new', 'shamelessly', 'watch', 'amelie', 'warm', 'drink', 'one', 'day', 'honest', 'id', 'get', 'nervous', 'everything', 'wasnt', 'much', 'point', 'anything', 'else', 'thats', 'id', 'never', 'thing', 'banging', 'noise', 'cry', 'help', 'lock', 'closet', 'quiet', 'sorry', 'rat', 'people', 'always', 'really', 'emotional', 'conversation', 'outside', 'apartment', 'rain', 'dramatic', 'maybe', 'im', 'extra', 'romantic', 'comedy', 'without', 'know', 'wish', 'spring', 'could', 'go', 'felt', 'weird', 'look', 'back', 'couple', 'sketchbook', 'ive', 'keep', 'werent', 'even', 'old', 'felt', 'distant', 'thing', 'id', 'draw', 'write', 'especially', 'im', 'second', 'breakfast', 'afternoon', 'cannot', 'stop', 'im', 'probably', 'bit', 'loon', 'like', 'kinglet', 'lot', 'theyre', 'probably', 'one', 'favorites', 'snipe', 'strange', 'theyre', 'great', 'also', 'really', 'like', 'whip', 'poor', 'will', 'nighthawks', 'maybe', 'occasionally', 'need', 'make', 'important', 'decision', 'enough', 'time', 'really', 'consider', 'maybe', 'little', 'thing', 'like', 'make', 'dinner', 'close', 'austin', 'sort', 'southern', 'narnia', 'city', 'seem', 'make', 'lot', 'effort', 'preserve', 'historical', 'thing', 'there', 'weird', 'little', 'pocket', 'magic', 'hidden', 'place', 'sometimes', 'make', 'puns', 'awful', 'im', 'secretly', 'proud', 'week', 'mainly', 'im', 'usually', 'good', 'make', 'pun', 'mind', 'actually', 'manage', 'put', 'one', 'together', 'sort', 'despite', 'one', 'professor', 'say', 'really', 'think', 'there', 'one', 'right', 'way', 'enjoy', 'book', 'fact', 'think', 'silly', 'look', 'nose', 'people', 'enjoy', 'book', 'plot', 'strain', 'relaxation', 'aggravation', 'long', 'walks', 'surrounded', 'nature', 'satisfy', 'way', 'ice', 'crackle', 'fresh', 'glass', 'water', 'id', 'forgotten', 'much', 'liked', 'place', 'feel', 'well', 'read', 'everyones', 'post', 'dont', 'know', 'relax', 'somehow', 'spent', 'hour', 'worry', 'treehouse', 'today', 'spaghetti', 'mm', 'mm', 'mm', 'speak', 'idea', 'make', 'dinner', 'tomorrow', 'ive', 'dug', 'deep', 'hole', 'every', 'piece', 'fruit', 'anyones', 'eat', 'spent', 'entire', 'grow', 'season', 'bath', 'sunlight', 'somewhere', 'comfort', 'idea', 'row', 'row', 'tree', 'full', 'fruit', 'thats', 'wait', 'ripen', 'im', 'aggressively', 'procrastinate', 'exactly', 'like', 'regular', 'procrastinate', 'im', 'aware', 'thing', 'need', 'im', 'sit', 'nothing', 'spite', 'im', 'thankful', 'legs', 'get', 'im', 'go', 'take', 'away', 'dont', 'want', 'im', 'put', 'thing', 'great', 'day', 'drape', 'thing', 'doze', 'park', 'bench', 'large', 'rock', 'sort', 'chair', 'cushion', 'also', 'ive', 'get', 'small', 'sack', 'full', 'world', 'seem', 'clean', 'day', 'winter', 'rain', 'thought', 'book', 'ive', 'read', 'recently', 'kind', 'binge', 'read', 'holiday', 'maybe', 'spoiler', 'elegance', 'hedgehog', 'kind', 'like', 'sort', 'like', 'read', 'feel', 'like', 'need', 'something', 'im', 'sure', 'oh', 'need', 'laundry', 'sort', 'way', 'something', 'feel', 'like', 'obvious', 'important', 'sense', 'want', 'take', 'dog', 'live', 'somewhere', 'alone', 'mountains', 'mile', 'away', 'anywhere', 'dont', 'know', 'exactly', 'song', 'help', 'calm', 'nothing', 'else', 'music', 'strange', 'brain', 'manage', 'want', 'crawl', 'dumpster', 'throw', 'trash', 'oh', 'god', 'realize', 'ive', 'become', 'im', 'hot', 'drink', 'spending', 'time', 'draw', 'read', 'even', 'sometimes', 'thunder', 'crack', 'outside', 'wonderful', 'perfect', 'way', 'end', 'day', 'blah', 'go', 'bad', 'time', 'hope', 'thing', 'bound', 'happen', 'every', 'awhile', 'fear', 'failure', 'joy', 'success', 'good', 'combo', 'want', 'melt', 'sink', 'earth', 'rebuild', 'stalagmite', 'somewhere', 'cool', 'empty', 'space', 'also', 'im', 'bad', 'meditate', 'yes', 'hadnt', 'think', 'feel', 'merry', 'already', 'think', 'pun', 'do', 'sargon', 'aww', 'haha', 'nice', 'know', 'somebody', 'else', 'salamander', 'trouble', 'proud', 'id', 'never', 'see', 'salamander', 'parking', 'lot', 'keep', 'walk', 'try', 'think', 'much', 'couldnt', 'leave', 'would', 'get', 'run', 'step'], tags='f'), TaggedDocument(words=['haha', 'wow', 'thats', 'interesting', 'sure', 'yeah', 'guess', 'im', 'ok', 'pixie', 'time', 'sometimes', 'wanna', 'get', 'foot', 'grounded', 'reality', 'principled', 'show', 'tough', 'love', 'erica', 'call', 'life', 'theyre', 'sooo', 'good', 'sense', 'present', 'real', 'life', 'real', 'problem', 'innovative', 'way', 'seem', 'like', 'every', 'time', 'wanna', 'get', 'know', 'extroverted', 'person', 'invite', 'meet', 'large', 'group', 'people', 'dont', 'know', 'loud', 'music', 'expect', 'okay', 'yeah', 'super', 'lovey', 'head', 'cloud', 'like', 'fairy', 'lol', 'yeah', 'remember', 'hug', 'use', 'mean', 'something', 'lol', 'im', 'use', 'think', 'sometimes', 'picture', 'forum', 'cult', 'pixy', 'offence', 'everyone', 'here', 'really', 'nice', 'haha', 'awwww', 'thanks', 'gettin', 'well', 'honestly', 'introverted', 'borderline', 'make', 'kind', 'awkward', 'feel', 'like', 'overload', 'cuteness', 'poetic', 'response', 'bad', 'lol', 'yeah', 'im', 'type', 'take', 'age', 'open', 'even', 'girl', 'im', 'comfortable', 'definitely', 'least', 'bunch', 'solid', 'friendly', 'chat', 'make', 'thing', 'whats', 'cuter', 'bunch', 'baby', 'tongue', 'post', 'baby', 'photo', 'dreamy', 'dubstep', 'alanis', 'morissette', 'really', 'beautiful', 'person', 'great', 'question', 'im', 'really', 'sure', 'think', 'ive', 'read', 'somewhere', 'product', 'upbringing', 'distribution', 'score', 'type', 'one', 'type', 'two', 'type', 'three', 'type', 'four', 'type', 'five', 'type', 'six', 'type', 'seven', 'type', 'eight', 'wow', 'beautiful', 'thanks', 'share', 'think', 'best', 'song', 'yet', 'artists', 'red', 'hot', 'chili', 'pepper', 'gyptian', 'skindred', 'skrillex', 'jason', 'mraz', 'song', 'cant', 'stop', 'red', 'hot', 'chili', 'pepper', 'meet', 'corner', 'red', 'hot', 'chili', 'pepper', 'feel', 'uncomfortable', 'shake', 'hand', 'like', 'meet', 'hold', 'hand', 'lol', 'summery', 'dance', 'pop', 'come', 'canada', 'man', 'theres', 'plenty', 'enough', 'go', 'around', 'canadian', 'girl', 'awesome', 'haha', 'guess', 'forget', 'forum', 'imaginary', 'world', 'girl', 'model', 'laugh', 'privacy', 'purpose', 'call', 'paranoid', 'lol', 'ah', 'heck', 'ill', 'post', 'new', 'one', 'im', 'surround', 'ghost', 'death', 'erica', 'seem', 'show', 'idealistic', 'ridiculous', 'also', 'awesome', 'happy', 'identify', 'luna', 'lovegood', 'shock', 'lol', 'havent', 'read', 'many', 'book', 'see', 'movie', 'im', 'thinkin', 'maybe', 'neville', 'ive', 'friend', 'compare', 'dobby', 'pretty', 'interesting', 'view', 'bad', 'thing', 'think', 'probably', 'bad', 'thing', 'youre', 'act', 'like', 'person', 'scenario', 'youre', 'damn', 'catchy', 'lol', 'becomes', 'star', 'centre', 'fielder', 'lead', 'world', 'series', 'ill', 'laugh', 'youll', 'remember', 'post', 'grin', 'edit', 'deal', 'huge', 'trade', 'bunch', 'blue', 'jay', 'steal', 'colby', 'rasmus', 'card', 'lol', 'yuppp', 'kudos', 'fashion', 'sense', 'im', 'glad', 'post', 'cuz', 'sound', 'lot', 'like', 'lol', 'border', 'use', 'everything', 'magically', 'turn', 'amazing', 'school', 'lol', 'im', 'university', 'tear', 'yuppp', 'natural', 'beauty', 'sure', 'femininity', 'shine', 'kinda', 'help', 'dont', 'cake', 'makeup', 'dont', 'subscribe', 'label', 'one', 'know', 'pretty', 'much', 'talk', 'drink', 'tho', 'lol', 'otherwise', 'wayyy', 'shy', 'softspoken', 'hipster', 'ish', 'artsy', 'temperament', 'put', 'ease', 'thats', 'hmm', 'weird', 'lol', 'consensus', 'scorpio', 'secretive', 'mysterious', 'hiding', 'within', 'scorpion', 'shell', 'scorpios', 'usually', 'hard', 'read', 'though', 'maybe', 'im', 'scorpio', 'kinda', 'fit', 'cuz', 'water', 'sign', 'scorpio', 'introvert', 'gotta', 'pretty', 'perceptive', 'scorpio', 'eye', 'red', 'hot', 'chili', 'pepper', 'new', 'single', 'leak', 'theyve', 'definitely', 'go', 'different', 'youre', 'welcome', 'guy', 'saw', 'live', 'john', 'really', 'hope', 'lifetime', 'read', 'interview', 'john', 'frusciante', 'probably', 'favourite', 'guitarist', 'say', 'seem', 'distinctly', 'unbelievably', 'mystic', 'general', 'life', 'philosophy', 'really', 'dancehall', 'metal', 'dubstep', 'drum', 'bass', 'haha', 'thanks', 'man', 'fun', 'try', 'amaze', 'amaze', 'song', 'sing', 'dance', 'heart', 'drinking', 'till', 'lose', 'inhibition', 'silly', 'close', 'friend', 'best', 'party', 'actually', 'birthday', 'house', 'cuz', 'get', 'actually', 'sound', 'familiar', 'except', 'make', 'thing', 'go', 'along', 'im', 'mood', 'dance', 'basically', 'look', 'like', 'spineless', 'michael', 'jackson', 'drug', 'really', 'dont', 'care', 'paraphrase', 'erica', 'care', 'people', 'think', 'let', 'care', 'people', 'think', 'get', 'way', 'parent', 'opinion', 'friend', 'opinion', 'etc', 'rez', 'senhorfrio', 'vanillabean', 'thanks', 'suggest', 'erica', 'watch', 'first', 'episodes', 'amazing', 'also', 'love', 'fact', 'show', 'toronto', 'recognize', 'place', 'ive', 'meaning', 'watch', 'show', 'marshall', 'look', 'like', 'self', 'conscious', 'isnt', 'really', 'confident', 'social', 'setting', 'outside', 'group', 'thing', 'introvert', 'girl', 'really', 'take', 'charge', 'push', 'comfort', 'zone', 'ive', 'experience', 'mostly', 'unsuccessful', 'ive', 'see', 'moreso'], tags='f')]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(dm=1, vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'feel' appeared 22469 times in the training corpus.\n",
      "Word 'think' appeared 49119 times in the training corpus.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word 'feel' appeared {model.wv.get_vecattr('feel', 'count')} times in the training corpus.\")\n",
    "print(f\"Word 'think' appeared {model.wv.get_vecattr('think', 'count')} times in the training corpus.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11344896 -0.4965007  -0.41206655 -0.13912763  0.17313938 -0.15619417\n",
      " -0.20826718  0.2086914  -0.4389367   0.27128014  0.13080177 -0.19327503\n",
      "  0.1321911   0.03304035 -0.05019431 -0.13650183  0.12333461  0.15993096\n",
      " -0.2727517  -0.22526816  0.29956594  0.05439552 -0.13513777 -0.20643973\n",
      "  0.00316308 -0.07599627  0.04502578  0.3710139   0.33218023 -0.20041418\n",
      "  0.1851082   0.21061155 -0.16035871  0.22556521 -0.13261831  0.0472893\n",
      "  0.36776415 -0.28573623  0.39763233 -0.32951465  0.01215485  0.01757574\n",
      "  0.25727788 -0.03497041 -0.14688474  0.11000869 -0.01370976 -0.12458228\n",
      "  0.18366544  0.0531315 ]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['unaware', 'people', 'dont', 'confidence', 'often', 'picture'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc_id in range(len(train_corpus)):\n",
    "#     inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "#     sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.index(value, start=0, stop=9223372036854775807, /)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [('t', 0.5856667757034302), ('f', 0.3399638831615448)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "0 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/westonshuken/Documents/Myers/personality-prediction/doc2vec.ipynb Cell 17'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/westonshuken/Documents/Myers/personality-prediction/doc2vec.ipynb#ch0000016?line=3'>4</a>\u001b[0m inferred_vector \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minfer_vector(train_corpus[doc_id]\u001b[39m.\u001b[39mwords)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/westonshuken/Documents/Myers/personality-prediction/doc2vec.ipynb#ch0000016?line=4'>5</a>\u001b[0m sims \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdv\u001b[39m.\u001b[39mmost_similar([inferred_vector], topn\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39mdv))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/westonshuken/Documents/Myers/personality-prediction/doc2vec.ipynb#ch0000016?line=5'>6</a>\u001b[0m rank \u001b[39m=\u001b[39m [docid \u001b[39mfor\u001b[39;49;00m docid, sim \u001b[39min\u001b[39;49;00m sims]\u001b[39m.\u001b[39;49mindex(doc_id)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/westonshuken/Documents/Myers/personality-prediction/doc2vec.ipynb#ch0000016?line=6'>7</a>\u001b[0m ranks\u001b[39m.\u001b[39mappend(rank)\n",
      "\u001b[0;31mValueError\u001b[0m: 0 is not in list"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    print(sims)\n",
    "    # rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    # ranks.append(rank)\n",
    "\n",
    "    # second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_for_learning(model, posts):\n",
    "    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in posts])\n",
    "    return targets, feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.7570308898109728\n"
     ]
    }
   ],
   "source": [
    "y_train, X_train = vector_for_learning(model, train_corpus)\n",
    "y_test, X_test = vector_for_learning(model, test_corpus)\n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f'Testing accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.7570308898109728\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f'Testing accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(dm=1, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, alpha=0.025, min_alpha=0.001)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_corpus)])\n",
    "train_corpus  = shuffle(train_corpus)\n",
    "model_dbow.train(train_corpus, total_examples=len(train_corpus), epochs=30)\n",
    "def vector_for_learning(model, input_docs):\n",
    "    sents = input_docs\n",
    "    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vector_for_learning(model, train_corpus)\n",
    "y_test, X_test = vector_for_learning(model, test_corpus)\n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f'Testing accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "632bafddcbad8152c10db2ae5692e228f3b2b87dc97f9f8213011cfcc1a717c5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
