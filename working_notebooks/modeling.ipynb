{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./finalmodeling_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned tokens tally up to 29870979 total words\n",
      "--------------------\n",
      "Each feature has on average 124.0 words\n",
      "--------------------\n",
      "Each feature has a median of 82.0 words\n",
      "--------------------\n",
      "The minimum post is 51 words\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_words = df['post_token'].apply(lambda x: len(x)).sum()\n",
    "print(f'The cleaned tokens tally up to {total_words} total words')\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "ave_post = df['post_token'].apply(lambda x: len(x)).mean()\n",
    "print(f'Each feature has on average {round(ave_post)} words')\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "med_post = df['post_token'].apply(lambda x: len(x)).median()\n",
    "print(f'Each feature has a median of {round(med_post)} words')\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "min_post = df['post_token'].apply(lambda x: len(x)).min()\n",
    "print(f'The minimum post is {round(min_post)} words')\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.hist(df['post_token'].apply(lambda x: len(x)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yup sound like rant honest first paragraph little hard follow youve got lot run sentence issue girlfriend might enfp know best shes u shes got shit learn organize specific straight forward word waver youve got shit learn listening feeling finding right way express know person understands understand personal priority finding common ground standard stuff people different underlying goal conversation instead infiltrating mind find common ground try explain side even harder make much sense yet personal bias mean every argument mix objective fact subjectively interpreted organized fit completely subjective goal frequently see v thing side conveniently ignore choice data others argument one party meltdown people say crap like person logic theyre stupid theyre emotionally cold insulting stuff either party get closer solution person shut ability listen adult hissy fit mean type frequently like say theyre hissy fit look cleaner still hissy fit dont know mature know going end mature two first listen offer gesture explain listen find others underlying goal situation value attached goal reliability harmony whatever theyll take step change future thought behavior surrounding situation theyll collect data even start compose side theyll repeat back accuracy check partner make sure got story right mean listen side satisfied complete together youll notice one stay immature theyll blind effort one good listener subtle soft quality thats work well soften harsh emotion dissolve hissy fit blockade mean block either end foster resentment screwed dynamic even stick nobody dream growing old someone nag time insult behind back like sitcom level cringey shit fantasize future life partner yet know couple state crap like dont like em love em theyre hopeless lead way rant taken grain salt become disrespect discontentment dead bedroom bleh im getting way ahead youre looking life advice thought thats stream conscious ne come mind reading passage based life experience ive hopefully worth thinking end youll chose likely path make something entirely new best luck\n",
      ">>>>>>>>>>enfp<<<<<<<<<<\n",
      "-------------------------\n",
      "fucked constantly as long time worst year life many thing tried failing every idea ne came rendered useless found something would fix situation something never going fail due simple sure would work safe bet last plan plan due extreme bad luck didnt work chilling girlfriend fun enjoying nice summer day decided check thing gone saw failed completely world shattered verge emotional breakdown didnt break cry girlfriend already worried enough gone pale matter second ne done best wasnt enough couldnt anymore turned felt lost tried come solution felt panic desperation instead luckily mother enfp saw affected everything happened year say told blame thing inflicted stupid decision ne doms best brainstormed idea one idea stood rest one idea saved dont know would happened hadnt said word wouldnt good ne shuts day ask another ne dom able help\n",
      ">>>>>>>>>>entp<<<<<<<<<<\n",
      "-------------------------\n",
      "another xcom remake fan havent played fe ive meaning really enjoyed game really recommend getting enemy within expansion old style expansion many dlcs add lot new content new alien new item new soldier skill tree making feel like new game without completely different doesnt feel like enemy unknown needed except new grenade alternative couple ui button playing never go back always trying get best outcome possible turn intensive rewarding nothing beat getting rid disc mooks flanking making shoot count dividing squad flank enemy group half sniper make max zone skill\n",
      ">>>>>>>>>>intp<<<<<<<<<<\n",
      "-------------------------\n",
      "yes overcome tremendous amount trauma im still wood yet let try explain real quick grew emotionally distant short tempered father never managed convey proud severely bullied peer primary school top mobbed teacher decided fail life withheld opportunity educated well found gay didnt bode well small rural village grew closeted boyfriend decided didnt need talk plan marry woman confronted blue fact two week wedding two year come led severe clinical depression left shattered piece semblance recovery started could least halfway function society cost college graduation though year busy working issue therapy session last one gave tool keep working still im currently stage forgiven tormenter teacher late ex boyfriend sake needed able move holding lot bitterness smouldering rage hatred tainted everything let touch wood yet recently noticed numbed point dont actually engage outside world anymore use nihilism misanthropy excuse absolve obligation human part world course one severely suffers stance flattened affect occasional severe bout ink black mood way living happy life next big thing tackle get fog get back state feeling longer suppressed word advice people toxic yes cutting tie beneficial careful though burn bridge people toxic make feel uncomfortable touch upon wound care come conclusion orient u towards people able effective dont shut part irony saying intp rare source self induced humour dont afraid feel processing emotion feeling might highly unpleasant feeling feeling like emotion psi could blow minute wonderful feeling afterwards\n",
      ">>>>>>>>>>intp<<<<<<<<<<\n",
      "-------------------------\n",
      "deeply disconcerting first wonder different looking picture watching old home video listening old recording deceased im torn thinking unhealthy way cope somehow feeling natural looking back old picture reflecting memory could hypothetically improved point indistinguishable actual person id say certainly cross line whether living deceased something function precisely would feel like type theft raise new question whether future would even logical love actual living human one day die one could love ai would never die\n",
      ">>>>>>>>>>intj<<<<<<<<<<\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# looking at some random posts\n",
    "for i in np.random.randint(0, len(df), 5):\n",
    "    print(df['token_joined'].iloc[i])\n",
    "    print('>'*10 + df['type'].iloc[i] + '<'*10)\n",
    "    print('-'*25)\n",
    "\n",
    "## BE CAREFUL... Reddit posts can be quite rude...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>post_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>enfp</td>\n",
       "      <td>[intjs, different, intriguing, drawn, girlfrie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>entj</td>\n",
       "      <td>[entj, thing, happen, back, august, intj, girl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>intp</td>\n",
       "      <td>[infj, friend, love, around, help, balance, em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>enfp</td>\n",
       "      <td>[enfp, spent, many, year, popular, flirtatious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>intp</td>\n",
       "      <td>[intp, look, lot, like, ne, ego, type, infanti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241536</th>\n",
       "      <td>infp</td>\n",
       "      <td>[enfj, close, female, friend, enfj, boyfriend,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241554</th>\n",
       "      <td>infp</td>\n",
       "      <td>[isfp, one, doesnt, doesnt, bother, intp, way,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241562</th>\n",
       "      <td>intj</td>\n",
       "      <td>[enfp, maybe, im, wondering, pewdiepie, used, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241609</th>\n",
       "      <td>intp</td>\n",
       "      <td>[intp, much, prefer, self, teaching, cant, foc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241613</th>\n",
       "      <td>infp</td>\n",
       "      <td>[infp, peep, get, jealous, often, jealous, mea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6241 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        type                                         post_token\n",
       "50      enfp  [intjs, different, intriguing, drawn, girlfrie...\n",
       "86      entj  [entj, thing, happen, back, august, intj, girl...\n",
       "94      intp  [infj, friend, love, around, help, balance, em...\n",
       "103     enfp  [enfp, spent, many, year, popular, flirtatious...\n",
       "160     intp  [intp, look, lot, like, ne, ego, type, infanti...\n",
       "...      ...                                                ...\n",
       "241536  infp  [enfj, close, female, friend, enfj, boyfriend,...\n",
       "241554  infp  [isfp, one, doesnt, doesnt, bother, intp, way,...\n",
       "241562  intj  [enfp, maybe, im, wondering, pewdiepie, used, ...\n",
       "241609  intp  [intp, much, prefer, self, teaching, cant, foc...\n",
       "241613  infp  [infp, peep, get, jealous, often, jealous, mea...\n",
       "\n",
       "[6241 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.width', 120)\n",
    "regxx = '(intp)|(intj)|(entp)|(entj)|(infj)|(infp)|(enfj)|(enfp)|(istj)|(isfj)|(estj)|(esfj)|(istp)|(isfp)|(estp)|(esfp)'\n",
    "cheat_index = df[df['token_joined'].str.match(regxx)]['token_joined'].index\n",
    "\n",
    "df.loc[cheat_index, ['type', 'post_token']]\n",
    "\n",
    "# It would be worth removing and MBTI text from the joined tokens even if it does not match the writers type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Linear Support Vector Machine\n",
    "sgd = Pipeline([('vect', CountVectorizer()), \n",
    "               ('clf', SGDClassifier())\n",
    "              ])\n",
    "\n",
    "# Baseline Naive Bayes\n",
    "naive = Pipeline([('vect', CountVectorizer()),\n",
    "               ('clf', MultinomialNB())\n",
    "              ])\n",
    "\n",
    "# Baseline Random Forest\n",
    "rfc = Pipeline([('vect', CountVectorizer()),\n",
    "               ('clf', RandomForestClassifier(max_depth=1000))\n",
    "              ])\n",
    "# Baseline Linear SVC \n",
    "lin_svc = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('clf', LinearSVC())\n",
    "                ])\n",
    "# Baseline SVC \n",
    "svc_ = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('clf', SVC())\n",
    "                ])\n",
    "\n",
    "''' THE FOLLOWING MODELS ARE THE SAME AS BASELINE BUT WITH BIGRAMS'''\n",
    "\n",
    "# Baseline Linear Support Vector Machine /w Bigrams\n",
    "sgd_n2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))), \n",
    "               ('clf', SGDClassifier())\n",
    "              ])\n",
    "\n",
    "# Baseline Naive Bayes /w Bigrams\n",
    "naive_n2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "               ('clf', MultinomialNB())\n",
    "              ])\n",
    "\n",
    "# Baseline Random Forest /w Bigrams\n",
    "rfc_n2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "            ('clf', RandomForestClassifier(max_depth=1000))\n",
    "              ])\n",
    "# Baseline Linear SVC w/ Bigrams\n",
    "lin_svc_n2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "                    ('clf', LinearSVC())\n",
    "                ])\n",
    "# Baseline SVC w/ Bigrams\n",
    "svc_n2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "                    ('clf', SVC())\n",
    "                ])\n",
    "\n",
    "\n",
    "# List of baseline models\n",
    "baseline_models = [sgd, naive, rfc, lin_svc, svc_, sgd_n2, naive_n2, rfc_n2, lin_svc_n2, svc_n2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['token_joined']\n",
    "y = df['type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipe in baseline_models:  \n",
    "    model_name = str(pipe[1])\n",
    "    print(f'Working on {model_name} @ {time.asctime()}')\n",
    "    print('-'*20)\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    cv_score = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "    cv_score_mean = round(np.mean(cv_score), 4)\n",
    "    \n",
    "    y_pred = pipe.predict(X_test)\n",
    "    acc_score = accuracy_score(y_pred, y_test)\n",
    "\n",
    "    print(f\"Model: {model_name} CV: {cv_score_mean} TEST ACC: {acc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' THE FOLLOWING MODELS LIMITING min doc frequency ON THE CountVecorizer'''\n",
    "\n",
    "# Baseline Linear Support Vector Machine\n",
    "sgd = Pipeline([('vect', CountVectorizer(min_df=20)), \n",
    "               ('clf', SGDClassifier())\n",
    "              ])\n",
    "\n",
    "# Baseline Naive Bayes\n",
    "naive = Pipeline([('vect', CountVectorizer(min_df=20)),\n",
    "               ('clf', MultinomialNB())\n",
    "              ])\n",
    "\n",
    "# Baseline Logistic Regression\n",
    "log_reg = Pipeline([('vect', CountVectorizer(min_df=20)),\n",
    "                    ('clf', LogisticRegression(max_iter=1000))\n",
    "              ])\n",
    "# Baseline Random Forest\n",
    "rfc = Pipeline([('vect', CountVectorizer(min_df=20)),\n",
    "               ('clf', RandomForestClassifier())\n",
    "              ])\n",
    "# Baseline Linear SVC \n",
    "lin_svc = Pipeline([('vect', CountVectorizer(min_df=20)),\n",
    "                    ('clf', LinearSVC())\n",
    "                ])\n",
    "# Baseline SVC \n",
    "svc_ = Pipeline([('vect', CountVectorizer(min_df=20)),\n",
    "                    ('clf', SVC())\n",
    "                ])\n",
    "\n",
    "''' THE FOLLOWING MODELS ARE THE SAME AS PRIOR BUT WITH BIGRAMS'''\n",
    "\n",
    "# Baseline Linear Support Vector Machine /w Bigrams\n",
    "sgd_n2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=20)), \n",
    "               ('clf', SGDClassifier())\n",
    "              ])\n",
    "\n",
    "# Baseline Naive Bayes /w Bigrams\n",
    "naive_n2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=20)),\n",
    "               ('clf', MultinomialNB())\n",
    "              ])\n",
    "\n",
    "# Baseline Logistic Regression /w Bigrams\n",
    "log_reg_n2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=20)),\n",
    "               ('clf', LogisticRegression(max_iter=1000))\n",
    "              ])\n",
    "# Baseline Random Forest /w Bigrams\n",
    "rfc_n2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=20)),\n",
    "               ('clf', RandomForestClassifier())\n",
    "              ])\n",
    "# Baseline Linear SVC w/ Bigrams\n",
    "lin_svc_n2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=20)),\n",
    "                    ('clf', LinearSVC())\n",
    "                ])\n",
    "# Baseline SVC w/ Bigrams\n",
    "svc_n2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=20)),\n",
    "                    ('clf', SVC())\n",
    "                ])\n",
    "\n",
    "\n",
    "# List of baseline models\n",
    "baseline_models = [sgd, naive, log_reg, rfc, lin_svc, svc_, sgd_n2, naive_n2, log_reg_n2, rfc_n2, lin_svc_n2, svc_n2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Linear Support Vector Machine\n",
    "sgd = Pipeline([('vect', TfidfVectorizer()), \n",
    "               ('clf', SGDClassifier())\n",
    "              ])\n",
    "\n",
    "# Baseline Naive Bayes\n",
    "naive = Pipeline([('vect', TfidfVectorizer()),\n",
    "               ('clf', MultinomialNB())\n",
    "              ])\n",
    "\n",
    "# Baseline Logistic Regression\n",
    "log_reg = Pipeline([('vect', TfidfVectorizer()),\n",
    "               ('clf', LogisticRegression())\n",
    "              ])\n",
    "# Baseline Random Forest\n",
    "rfc = Pipeline([('vect', TfidfVectorizer()),\n",
    "               ('clf', RandomForestClassifier())\n",
    "              ])\n",
    "# Baseline Linear SVC \n",
    "lin_svc = Pipeline([('vect', TfidfVectorizer()),\n",
    "                    ('clf', LinearSVC())\n",
    "                ])\n",
    "# Baseline SVC \n",
    "svc_ = Pipeline([('vect', TfidfVectorizer()),\n",
    "                    ('clf', SVC())\n",
    "                ])\n",
    "\n",
    "''' THE FOLLOWING MODELS ARE THE SAME AS BASELINE BUT WITH BIGRAMS'''\n",
    "\n",
    "# Baseline Linear Support Vector Machine /w Bigrams\n",
    "sgd_n2 = Pipeline([('vect', TfidfVectorizer(ngram_range=(1,2))), \n",
    "               ('clf', SGDClassifier())\n",
    "              ])\n",
    "\n",
    "# Baseline Naive Bayes /w Bigrams\n",
    "naive_n2 = Pipeline([('vect', TfidfVectorizer(ngram_range=(1,2))),\n",
    "               ('clf', MultinomialNB())\n",
    "              ])\n",
    "\n",
    "# Baseline Logistic Regression /w Bigrams\n",
    "log_reg_n2 = Pipeline([('vect', TfidfVectorizer(ngram_range=(1,2))),\n",
    "               ('clf', LogisticRegression())\n",
    "              ])\n",
    "# Baseline Random Forest /w Bigrams\n",
    "rfc_n2 = Pipeline([('vect', TfidfVectorizer(ngram_range=(1,2))),\n",
    "               ('clf', RandomForestClassifier())\n",
    "              ])\n",
    "# Baseline Linear SVC w/ Bigrams\n",
    "lin_svc_n2 = Pipeline([('vect', TfidfVectorizer(ngram_range=(1,2))),\n",
    "                    ('clf', LinearSVC())\n",
    "                ])\n",
    "# Baseline SVC w/ Bigrams\n",
    "svc_n2 = Pipeline([('vect', TfidfVectorizer(ngram_range=(1,2))),\n",
    "                    ('clf', SVC())\n",
    "                ])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "033c84df5fb4c613acf884834f63930b25da6784759ce0fb831a430fcd673895"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('learn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
