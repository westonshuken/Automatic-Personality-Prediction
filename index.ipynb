{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Myers-Briggs Personality Prediction\n",
    "\n",
    "#### by Weston Shuken\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The purpose of this project is to use machine learning algorithms to precict the personality type of a person given their written text in English. The personality type predictions are based on the Myers-Briggs Type Indicator (MBTI) test as the target variable. The features or predictor variables are comments and posts from userson [PersonalityCafe](https://www.personalitycafe.com/). These posts and comments come from users who have explicitley labeled their MBTI personality on their profile. \n",
    "\n",
    "The Myers-Briggs test is a very popular test that ask users approximately 90 questions about their behavior and assigns the user a type of personality based on this assessment. This test takes around 20-30 for someone to complete. \n",
    "\n",
    "There are 16 different personality types using a combination of 8 overall traits. See below:\n",
    "\n",
    "    Introversion (I) vs Extroversion (E)\n",
    "    Intuition (N) vs Sensing (S)\n",
    "    Thinking (T) vs Feeling (F)\n",
    "    Judging (J) vs Perceiving (P)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for EDA, Cleaning, Plotting & Modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from mbti import run_models, preprocess\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Random Seed for everything\n",
    "SEED = 53188535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Data\n",
    "cafe_df = pd.read_csv('data/mbti_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_df['type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.bar(cafe_df['type'].value_counts(normalize=True).index, cafe_df['type'].value_counts(normalize=True).values*100)\n",
    "plt.title('Class Imbalance by percentage')\n",
    "plt.ylabel('%');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at a random posts\n",
    "rint = np.random.randint(0, len(cafe_df))\n",
    "print('>'*10 + cafe_df['type'].iloc[rint] + '<'*10)\n",
    "print('-'*25)\n",
    "print(cafe_df['posts'].iloc[rint][:1000])\n",
    "\n",
    "## BE CAREFUL... Reddit posts can sometimes be quite rude..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways from seeing the dataset:\n",
    "\n",
    "- The classes are quite imbalanced, which can bias the model to choosing one class over the other just given class weight.\n",
    "- There is a lot of cleaning to do with the text (lowercase, remove urls, remove symbols/punctuation, lemmitization, etc.)\n",
    "- There are definitely incorrect spellings of words and acroynms like `lol` & `btw`\n",
    "- The posts might include the personality type in them, which could be considered `data leakage`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning & Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean = cafe_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase y variables\n",
    "cafe_clean['type'] = cafe_clean.type.str.lower()\n",
    "\n",
    "# Creating list of the targets\n",
    "mbti_lst = list(set(cafe_clean['type'].values))\n",
    "\n",
    "# Lowercase X varaible onto a new column\n",
    "cafe_clean['clean_posts'] = cafe_clean['posts'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bringing in the prepreoccess class from mbti.py\n",
    "# This class provides functions to clean and tokenize our text data\n",
    "prepro = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "for mbti in mbti_lst:\n",
    "    plt.bar(str(mbti), len(cafe_clean[cafe_clean['clean_posts'].str.contains(mbti)].index))\n",
    "    plt.title('Data Leakage \\n target within predictors')\n",
    "    plt.ylabel('counts')\n",
    "    # print(cafe_clean[cafe_clean['clean_posts'].str.contains(mbti)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the target names in the actual posts\n",
    "cafe_clean['clean_posts'] = cafe_clean['clean_posts'].apply(lambda x: prepro.replace_mbti(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mbti in mbti_lst:\n",
    "    print(cafe_clean[cafe_clean['clean_posts'].str.contains(mbti)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The posts contain multiple posts seperated by 3 pipes '|||' w/ no spaces between. \n",
    "# This function will remove pipes and replace with a space.\n",
    "cafe_clean['clean_posts'] = cafe_clean['clean_posts'].apply(lambda x: prepro.pipe_remove(x))\n",
    "\n",
    "# This funciton will remove URLs in the posts\n",
    "cafe_clean['clean_posts'] = cafe_clean['clean_posts'].apply(lambda x: prepro.url_remove(x))\n",
    "\n",
    "# This function will remove punctuation (dependent on what is passed in). This has `/``, `_`, `:` \n",
    "cafe_clean['clean_posts'] = cafe_clean['clean_posts'].apply(lambda x: prepro.punc_remove(x))\n",
    "\n",
    "# Removes all characters that are not American Standard Code for Information Interchange\n",
    "cafe_clean['clean_posts'] = cafe_clean['clean_posts'].apply(lambda x: prepro.remove_symbols(x))\n",
    "\n",
    "# Fixes all spelling errors\n",
    "# cafe_clean['clean_posts'] = cafe_clean['clean_posts'].apply(lambda x: prepro.spelling(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create a column of cleaned words that have been tokenized.\n",
    "pattern = r\"(?u)\\b\\w\\w+\\b\" # words with more than 2 letters\n",
    "tokenizer = RegexpTokenizer(pattern) # instantiate tokenizer\n",
    "cafe_clean['post_tokens'] = cafe_clean['clean_posts'].apply(tokenizer.tokenize) # Tokenize to new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing any remaining numeric digits\n",
    "cafe_clean['post_tokens'] = cafe_clean['post_tokens'].apply(lambda x: prepro.remove_dig_token(x))\n",
    "\n",
    "# Removing stopwords\n",
    "cafe_clean['post_tokens'] = cafe_clean['post_tokens'].apply(lambda x: prepro.remove_stopwords(x))\n",
    "\n",
    "# Lemmatizing the words with POS tagging\n",
    "cafe_clean['post_tokens'] = cafe_clean['post_tokens'].apply(lambda x: prepro.lemmend_pos(x, pos=False)) # If true, takes a while (3mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the tokens together into one long string\n",
    "cafe_clean['joined_tokens'] = cafe_clean['post_tokens'].apply(lambda x: prepro.join_tokens(x)) # Creating new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing what characters are in the cleaned text vs. the pre-cleaned text\n",
    "clean_corpus = ' '.join(cafe_clean['joined_tokens'])\n",
    "print(f'CLEANED: {\"\".join(sorted(set(clean_corpus.lower())))}')\n",
    "print('-'*25)\n",
    "print('-'*25)\n",
    "\n",
    "\n",
    "corpus = ' '.join(cafe_df['posts'])\n",
    "print(f'PRE-CLEANED: {\"\".join(sorted(set(corpus.lower())))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = cafe_clean['posts'].apply(lambda x: len(x.split())).sum()\n",
    "print(f'The pre-cleaned tokens tally up to {total_words} total words')\n",
    "\n",
    "clean_words = cafe_clean['post_tokens'].apply(lambda x: len(x)).sum()\n",
    "print(f'The cleaned tokens tally up to {clean_words} total words')\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "ave_post = cafe_clean['posts'].apply(lambda x: len(x.split())).mean()\n",
    "print(f'Each feature in pre-cleaned has on average {round(ave_post)} words')\n",
    "\n",
    "ave_cleaned = cafe_clean['post_tokens'].apply(lambda x: len(x)).mean()\n",
    "print(f'Each feature in cleaned has on average {round(ave_cleaned)} words')\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "med_post = cafe_clean['posts'].apply(lambda x: len(x.split())).median()\n",
    "print(f'Each feature in pre-cleaned  has a median of {round(med_post)} words')\n",
    "\n",
    "med_cleaned = cafe_clean['post_tokens'].apply(lambda x: len(x)).median()\n",
    "print(f'Each feature has a median of {round(med_cleaned)} words')\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "min_post = cafe_clean['posts'].apply(lambda x: len(x.split())).min()\n",
    "print(f'The minimum post in pre-cleaned  is {round(min_post)} words')\n",
    "\n",
    "min_cleaned = cafe_clean['post_tokens'].apply(lambda x: len(x)).min()\n",
    "print(f'The minimum post in cleaned is {round(min_cleaned)} words')\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "max_post = cafe_clean['posts'].apply(lambda x: len(x.split())).max()\n",
    "print(f'The minimum post in pre-cleaned  is {round(max_post)} words')\n",
    "\n",
    "max_cleaned = cafe_clean['post_tokens'].apply(lambda x: len(x)).max()\n",
    "print(f'The minimum post in cleaned is {round(max_cleaned)} words')\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.hist(cafe_clean['post_tokens'].apply(lambda x: len(x)), label='cleaned', alpha=.5, bins=100)\n",
    "plt.hist(cafe_clean['posts'].apply(lambda x: len(x.split())), label='pre-cleaned', alpha=.5, bins=100)\n",
    "plt.axvline(ave_post, color='k', linestyle='dashed', linewidth=3, label='pre-cleaned mean')\n",
    "plt.axvline(ave_cleaned, color='r', linestyle='dashed', linewidth=3, label='cleaned mean')\n",
    "plt.legend()\n",
    "plt.title('Distribution of Post Length \\n Clean vs Pre-Cleaned');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rint = np.random.randint(0, len(cafe_df))\n",
    "print('>'*10 + cafe_df['type'].iloc[rint] + '<'*10)\n",
    "print('-'*25)\n",
    "print(cafe_clean['posts'].iloc[rint][:1000])\n",
    "print('-'*25)\n",
    "print(cafe_clean['joined_tokens'].iloc[rint][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "- A lot of words were reduced or eliminated that did not provide any use.\n",
    "- The dataset still has posts with very little words. Might need to drop these.\n",
    "- The lemmitization did not work well on some verbs and the `MBTI` replacement often is followed by an `s`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Binary Targets\n",
    "cafe_clean['i/e'] = cafe_clean['type'].map(lambda x: x[0])\n",
    "cafe_clean['n/s'] = cafe_clean['type'].map(lambda x: x[1])\n",
    "cafe_clean['t/f'] = cafe_clean['type'].map(lambda x: x[2])\n",
    "cafe_clean['p/j'] = cafe_clean['type'].map(lambda x: x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cafe_clean['i/e'].value_counts())\n",
    "print(cafe_clean['n/s'].value_counts())\n",
    "print(cafe_clean['t/f'].value_counts())\n",
    "print(cafe_clean['p/j'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.title('Weights Between Binary Classes')\n",
    "plt.ylabel('counts')\n",
    "for column in cafe_clean.columns[-4:]:\n",
    "    plt.bar(cafe_clean[column].value_counts().index, cafe_clean[column].value_counts().values, label=column)\n",
    "\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis w/ VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.polarity_scores(cafe_clean.loc[2,'joined_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['sentiment'] = cafe_clean['joined_tokens'].apply(lambda x: sid.polarity_scores(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['sentiment_total'] = cafe_clean['sentiment'].apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['sentiment_score'] = cafe_clean['sentiment_total'].apply(lambda x: '+' if x >= 0 else '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {}\n",
    "for mbti in mbti_lst:\n",
    "    data = cafe_clean[cafe_clean['type'] == mbti]\n",
    "    print(f'Average Sentiemnt Score for {mbti}: {data.sentiment_total.mean()}')\n",
    "    scores_dict.update({mbti: data.sentiment_total.mean()}) \n",
    "\n",
    "scores_dict = {k: v for k, v in sorted(scores_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "x = list(scores_dict.keys())\n",
    "height = list(scores_dict.values())\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(x, height);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {}\n",
    "for mbti in ['t', 'f']:\n",
    "    data = cafe_clean[cafe_clean['t/f'] == mbti]\n",
    "    print(f'Average Sentiemnt Score for {mbti}: {data.sentiment_total.mean()}')\n",
    "    scores_dict.update({mbti: data.sentiment_total.mean()}) \n",
    "\n",
    "scores_dict = {k: v for k, v in sorted(scores_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "x = list(scores_dict.keys())\n",
    "height = list(scores_dict.values())\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(x, height);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['polarity'] = cafe_clean['joined_tokens'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['subjectivity'] = cafe_clean['joined_tokens'].apply(lambda x: TextBlob(x).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "for mbti in mbti_lst:\n",
    "    plt.hist(cafe_clean['polarity'][cafe_clean['type'] == mbti], bins=100, alpha=.5, label=mbti)\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "for mbti in mbti_lst:\n",
    "    plt.hist(cafe_clean['subjectivity'][cafe_clean['type'] == mbti], bins=100, alpha=.5, label=mbti)\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {}\n",
    "for mbti in mbti_lst:\n",
    "    data = cafe_clean[cafe_clean['type'] == mbti]\n",
    "    print(f'Average Sentiemnt Score for {mbti}: {data.polarity.mean()}')\n",
    "    scores_dict.update({mbti: data.polarity.mean()}) \n",
    "\n",
    "scores_dict = {k: v for k, v in sorted(scores_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "x = list(scores_dict.keys())\n",
    "height = list(scores_dict.values())\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(x, height);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {}\n",
    "for mbti in mbti_lst:\n",
    "    data = cafe_clean[cafe_clean['type'] == mbti]\n",
    "    print(f'Average Sentiemnt Score for {mbti}: {data.subjectivity.mean()}')\n",
    "    scores_dict.update({mbti: data.subjectivity.mean()}) \n",
    "\n",
    "scores_dict = {k: v for k, v in sorted(scores_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "x = list(scores_dict.keys())\n",
    "height = list(scores_dict.values())\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(x, height);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {}\n",
    "for mbti in ['t', 'f']:\n",
    "    data = cafe_clean[cafe_clean['t/f'] == mbti]\n",
    "    print(f'Average Sentiemnt Score for {mbti}: {data.polarity.mean()}')\n",
    "    scores_dict.update({mbti: data.polarity.mean()}) \n",
    "\n",
    "scores_dict = {k: v for k, v in sorted(scores_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "x = list(scores_dict.keys())\n",
    "height = list(scores_dict.values())\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(x, height);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {}\n",
    "for mbti in ['t', 'f']:\n",
    "    data = cafe_clean[cafe_clean['t/f'] == mbti]\n",
    "    print(f'Average Sentiemnt Score for {mbti}: {data.subjectivity.mean()}')\n",
    "    scores_dict.update({mbti: data.subjectivity.mean()}) \n",
    "\n",
    "scores_dict = {k: v for k, v in sorted(scores_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "x = list(scores_dict.keys())\n",
    "height = list(scores_dict.values())\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(x, height);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Susan Li\n",
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_trigram(cafe_clean['joined_tokens'], 20)\n",
    "# for word, freq in common_words:\n",
    "#     print(word, freq)\n",
    "df_tgrams = pd.DataFrame(common_words, columns = ['Posts' , 'count'])\n",
    "df_tgrams = df_tgrams.groupby('Posts').sum()['count'].sort_values(ascending=True)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.barh(df_tgrams.index, df_tgrams.values);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(str(cafe_clean['joined_tokens']))\n",
    "pos_df = pd.DataFrame(blob.tags, columns = ['word' , 'pos'])\n",
    "pos_df = pos_df.pos.value_counts()[:20]\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.barh(pos_df.index, pos_df.values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Amount of chars in cropus: {cafe_clean['posts'].apply(lambda x: len(x)).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Amount of words in corpus: {cafe_clean['post_tokens'].apply(lambda x: len(x)).sum()}\")\n",
    "print(f\"Amount of chars in cropus: {cafe_clean['joined_tokens'].apply(lambda x: len(x)).sum()}\")\n",
    "print(f\"Average word length:\\\n",
    " {cafe_clean['joined_tokens'].apply(lambda x: len(x)).sum() / cafe_clean['post_tokens'].apply(lambda x: len(x)).sum()} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cafe_clean.to_csv('./data/cafe_clean_pos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Modeling\n",
    "\n",
    "There will be 4 sections of modeling:\n",
    "\n",
    "1. Multiclass\n",
    "2. Multiclass balanced classes using undersampling\n",
    "3. Binary\n",
    "4. Binary balanced classes using undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean = pd.read_csv('./data/cafe_clean_pos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "baseline_models = {'SGDClassifier': SGDClassifier(class_weight='balanced', random_state=SEED, loss=\"modified_huber\"),\n",
    "\n",
    "                  'LogisticRegression': LogisticRegression(n_jobs=1, C=.001),\n",
    "\n",
    "                  'NaiveBayes': MultinomialNB(),\n",
    "               \n",
    "                #   'DecisionTree': DecisionTreeClassifier(random_state=SEED),\n",
    "               \n",
    "                  'RandomForest': RandomForestClassifier(max_depth=8, random_state=SEED)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in baseline_models:\n",
    "    print(baseline_models[clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_table = pd.DataFrame(columns=['Model', 'Target', 'CVScore', 'TestAcc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_cv = rm.run(cafe_clean, 'joined_tokens', ['type'], baseline_models, models_table, tfidf=False, SEED=234)\n",
    "multiclass_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_tfidf = rm.run(cafe_clean, 'joined_tokens', ['type'], baseline_models, models_table, tfidf=True, SEED=234)\n",
    "multiclass_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.barh('model_accuracy', .4108)\n",
    "plt.barh(cafe_clean['type'].value_counts(normalize=True).index, cafe_clean['type'].value_counts(normalize=True).values)\n",
    "plt.xlim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Models Results\n",
    "The largest class is `infp` of 21%. The models are not performing at a useful level, however there is reason to believe that the models are finding some distinction in word usage between the different MBTI types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_rus_cv = rm.run_usampled(cafe_clean, 'joined_tokens', ['type'], baseline_models, models_table, tfidf=False, SEED=234)\n",
    "multiclass_rus_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_rus_tfidf = rm.run_usampled(cafe_clean, 'joined_tokens', ['type'], baseline_models, models_table, tfidf=True, SEED=234)\n",
    "multiclass_rus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.barh('model_accuracy', .1143)\n",
    "plt.barh('class balance', 1/16)\n",
    "plt.xlim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_cv['other'] = 'countvec'\n",
    "multiclass_tfidf['other'] = 'tfidf'\n",
    "multiclass_rus_cv['other'] = 'rus_countvec'\n",
    "multiclass_rus_tfidf['other'] = 'rus_tfidf'\n",
    "\n",
    "\n",
    "base_mc = pd.concat([multiclass_cv, multiclass_tfidf, multiclass_rus_cv, multiclass_rus_tfidf])\n",
    "base_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Undersampled Models Results\n",
    "The largest class is `infp` of 21%. The models are not performing at a useful level, however there is reason to believe that the models are finding some distinction in word usage between the different MBTI types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_targets = ['i/e', 'n/s', 't/f', 'p/j']\n",
    "bin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_cv = rm.run(cafe_clean, 'joined_tokens', bin_targets, baseline_models, models_table, tfidf=False, SEED=234)\n",
    "bin_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_tfidf = rm.run(cafe_clean, 'joined_tokens', bin_targets, baseline_models, models_table, tfidf=True, SEED=234)\n",
    "bin_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15,10), sharex=True)\n",
    "axs[0, 0].barh('accuracy', .78)\n",
    "axs[0, 0].barh(cafe_clean['i/e'].value_counts(normalize=True).index, cafe_clean['i/e'].value_counts(normalize=True).values)\n",
    "\n",
    "axs[0,1].barh('accuracy', .85)\n",
    "axs[0,1].barh(cafe_clean['n/s'].value_counts(normalize=True).index, cafe_clean['n/s'].value_counts(normalize=True).values)\n",
    "\n",
    "axs[1,0].barh('accuracy', .79)\n",
    "axs[1,0].barh(cafe_clean['t/f'].value_counts(normalize=True).index, cafe_clean['t/f'].value_counts(normalize=True).values)\n",
    "\n",
    "axs[1,1].barh('accuracy', .68)\n",
    "axs[1,1].barh(cafe_clean['p/j'].value_counts(normalize=True).index, cafe_clean['p/j'].value_counts(normalize=True).values)\n",
    "\n",
    "acc = [.78, .85, .79, .68]\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.title('Accuracy Minus Highest Class Weight')\n",
    "for i, target in enumerate(bin_targets):\n",
    "    increase = round(acc[i] - cafe_clean[target].value_counts(normalize=True)[0], 2)\n",
    "    print(f'Accuracy increase from class balance in {target} is: {increase}')\n",
    "    plt.bar(target, increase)\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_rus_cv = rm.run_usampled(cafe_clean, 'joined_tokens', bin_targets, baseline_models, models_table, tfidf=False, SEED=234)\n",
    "bin_rus_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_rus_tfidf = rm.run_usampled(cafe_clean, 'joined_tokens', bin_targets, baseline_models, models_table, tfidf=True, SEED=234)\n",
    "bin_rus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability we are seeing these results given a 50/50 class balance\n",
    "\n",
    "print(f\"Pobability of i/e model: {1 - binom.cdf(k=(1499*2)*.68, n=1499*2, p=0.5)}\")\n",
    "print(f\"Pobability of n/s model: {1 - binom.cdf(k=(898*2)*.67, n=(898*2), p=0.5)}\")\n",
    "print(f\"Pobability of t/f model: {1 - binom.cdf(k=(1499*2)*.80, n=1499*2, p=0.5)}\")\n",
    "print(f\"Pobability of p/j model: {1 - binom.cdf(k=(2575*2)*.67, n=2575*2, p=0.5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15,10), sharex=True)\n",
    "axs[0, 0].barh('accuracy', .68)\n",
    "axs[0, 0].barh(['i', 'e'], [.5, .5])\n",
    "\n",
    "axs[0,1].barh('accuracy', .73)\n",
    "axs[0,1].barh(['n', 's'], [.5, .5])\n",
    "\n",
    "axs[1,0].barh('accuracy', .8)\n",
    "axs[1, 0].barh(['t', 'f'], [.5, .5])\n",
    "\n",
    "axs[1,1].barh('accuracy', .66)\n",
    "axs[1,1].barh(['p', 'j'], [.5, .5])\n",
    "\n",
    "acc = [.68, .73, .8, .66]\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.title('Accuracy Minus Highest Class Weight')\n",
    "for i, target in enumerate(bin_targets):\n",
    "    print(f'Accuracy increase from class balance in {target} is: {round(acc[i] - .5, 2)}')\n",
    "    plt.bar(target, round(acc[i] - .5, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cafe_clean['t/f'] = cafe_clean['t/f'].replace({'t': 0, 'f': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cafe_clean.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Splitting to test and train sets\n",
    "# X = cafe_clean['joined_tokens']\n",
    "# y = cafe_clean['t/f']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "#                                                     y, \n",
    "#                                                     test_size= 0.25,\n",
    "#                                                     stratify=y,\n",
    "#                                                     random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vectorizing and transforming with TfidfVectorizer\n",
    "# tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# X_train_vec = tfidf.fit_transform(X_train)\n",
    "# X_test_vec = tfidf.transform(X_test)\n",
    "\n",
    "# # View the shapes of X variables\n",
    "# print('Shape of train features tensor:', X_train_vec.shape)\n",
    "# print('Shape of test features tensor:', X_test_vec.shape)\n",
    "\n",
    "# # View the shapes of y variables\n",
    "# print('Shape of train label tensor:', y_train.shape)\n",
    "# print('Shape of test label tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Dropout, Embedding, SpatialDropout1D, LSTM, Dense\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating Neural Network Model\n",
    "# model = Sequential()\n",
    "# model.add(layers.Dense(50, input_dim=(1771357), activation='relu'))\n",
    "# model.add(Dropout(.5))\n",
    "# model.add(layers.Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compiling and running Neural Network\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(X_train_vec.toarray(), np.array(y_train), epochs=5, batch_size=32, validation_split=.20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Printing test score accuracy\n",
    "# results = model.evaluate(X_test_vec.toarray(), np.array(y_test), batch_size=10)\n",
    "# print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting loss of train and validation sets by epochs\n",
    "# plt.figure(figsize=(8,5))\n",
    "# plt.title('Loss')\n",
    "# plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='validation')\n",
    "# plt.legend()\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting accuracy of train and validatio sets by epochs\n",
    "# plt.figure(figsize=(8,5))\n",
    "# plt.title('Accuracy')\n",
    "# plt.plot(history.history['accuracy'], label='train')\n",
    "# plt.plot(history.history['val_accuracy'], label='validation')\n",
    "# plt.legend()\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Appending results to metric table\n",
    "# NN_dict = {'Model': 'NeuralNetwork', \n",
    "#             'CV Score': max(history.history['val_accuracy']),\n",
    "#             'Test Accuracy': results[1],\n",
    "#             'Type': 'multiclass_NN'}\n",
    "# NN_row = pd.DataFrame(NN_dict, index=[0])\n",
    "\n",
    "# metric_table_multi = pd.concat([metric_table_multi, NN_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean = cafe_clean.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(cafe_clean, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True, drop=True)\n",
    "test_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_gens(dataframe):\n",
    "    for i, post in enumerate(dataframe['joined_tokens']):\n",
    "        tokens = gensim.utils.simple_preprocess(post)\n",
    "        yield gensim.models.doc2vec.TaggedDocument(tokens, dataframe.loc[i, 't/f'])\n",
    "\n",
    "train_corpus = list(tokenize_gens(train_df))\n",
    "test_corpus = list(tokenize_gens(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(dm=1, \n",
    "                                    vector_size=50, \n",
    "                                    min_count=2, \n",
    "                                    epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Word 'feel' appeared {model.wv.get_vecattr('feel', 'count')} times in the training corpus.\")\n",
    "# print(f\"Word 'think' appeared {model.wv.get_vecattr('think', 'count')} times in the training corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.infer_vector(['unaware', 'people', 'dont', 'confidence', 'often', 'picture'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_for_learning(model, posts):\n",
    "    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in posts])\n",
    "    return targets, feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vector_for_learning(model, train_corpus)\n",
    "y_test, X_test = vector_for_learning(model, test_corpus)\n",
    "\n",
    "clf = SGDClassifier(class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f'Testing accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=.0001, class_weight='balanced')\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(f'Testing accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=1000, max_depth=25)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "print(f'Testing accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean = pd.read_csv('./data/cafe_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Pipeline([('vec', TfidfVectorizer(ngram_range=(1,2))),\n",
    "    ('clf', SGDClassifier(class_weight='balanced', random_state=SEED, loss=\"modified_huber\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in bin_targets:\n",
    "    X = cafe_clean['joined_tokens']\n",
    "    y = cafe_clean[target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=SEED)\n",
    "\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    acc_score = accuracy_score(y_pred, y_test)\n",
    "\n",
    "    print(classification_report(y_pred=y_pred, y_true=y_test, target_names=cafe_clean[target].unique()))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    plt.rcParams.update({'font.size': 10})\n",
    "    plt.rc('xtick', labelsize=10) \n",
    "    plt.rc('ytick', labelsize=10) \n",
    "    plot_confusion_matrix(final_model, X_test, y_test, ax=ax, normalize='true')\n",
    "    plt.grid(False);\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i/e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['i/e'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_t = Pipeline([('vec', TfidfVectorizer(ngram_range=(1,2))),\n",
    "                ('clf', SGDClassifier(class_weight={'s': .77, 'n': .23}, random_state=SEED,  loss=\"modified_huber\"))\n",
    "            ])\n",
    "\n",
    "X = cafe_clean['joined_tokens']\n",
    "y = cafe_clean['i/e']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=SEED)\n",
    "\n",
    "sgd_t.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgd_t.predict(X_test)\n",
    "acc_score = accuracy_score(y_pred, y_test)\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test, target_names=cafe_clean['i/e'].unique()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=10) \n",
    "plot_confusion_matrix(sgd_t, X_test, y_test, ax=ax, normalize='true')\n",
    "plt.grid(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['n/s'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_t = Pipeline([('vec', TfidfVectorizer(ngram_range=(1,2))),\n",
    "                ('clf', SGDClassifier(class_weight={'s': .90, 'n': .10}, random_state=SEED,  loss=\"modified_huber\"))\n",
    "            ])\n",
    "\n",
    "X = cafe_clean['joined_tokens']\n",
    "y = cafe_clean['n/s']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=SEED)\n",
    "\n",
    "sgd_t.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgd_t.predict(X_test)\n",
    "acc_score = accuracy_score(y_pred, y_test)\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test, target_names=cafe_clean['n/s'].unique()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=10) \n",
    "plot_confusion_matrix(sgd_t, X_test, y_test, ax=ax, normalize='true')\n",
    "plt.grid(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t/f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['t/f'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_t = Pipeline([('vec', TfidfVectorizer(ngram_range=(1,2))),\n",
    "                ('clf', SGDClassifier(class_weight={'f': .45, 't': .55}, random_state=SEED,  loss=\"modified_huber\"))\n",
    "            ])\n",
    "\n",
    "X = cafe_clean['joined_tokens']\n",
    "y = cafe_clean['t/f']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=SEED)\n",
    "\n",
    "sgd_t.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgd_t.predict(X_test)\n",
    "acc_score = accuracy_score(y_pred, y_test)\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test, target_names=cafe_clean['t/f'].unique()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=10) \n",
    "plot_confusion_matrix(sgd_t, X_test, y_test, ax=ax, normalize='true')\n",
    "plt.grid(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p/j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean['p/j'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_t = Pipeline([('vec', TfidfVectorizer(ngram_range=(1,2), min_df=50)),\n",
    "                ('clf', SGDClassifier(class_weight={'p': .40, 'j': .60}, random_state=SEED,  loss=\"modified_huber\"))\n",
    "            ])\n",
    "\n",
    "X = cafe_clean['joined_tokens']\n",
    "y = cafe_clean['p/j']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=SEED)\n",
    "\n",
    "sgd_t.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgd_t.predict(X_test)\n",
    "acc_score = accuracy_score(y_pred, y_test)\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test, target_names=cafe_clean['p/j'].unique()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=10) \n",
    "plot_confusion_matrix(sgd_t, X_test, y_test, ax=ax, normalize='true')\n",
    "plt.grid(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_clean = pd.read_csv('./data/cafe_clean.csv')\n",
    "cafe_clean.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           f       0.80      0.79      0.80      1152\n",
      "           t       0.77      0.78      0.77      1017\n",
      "\n",
      "    accuracy                           0.79      2169\n",
      "   macro avg       0.79      0.79      0.79      2169\n",
      "weighted avg       0.79      0.79      0.79      2169\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_model = Pipeline([('vec', TfidfVectorizer(ngram_range=(1,2), min_df=50)),\n",
    "                ('clf', SGDClassifier(class_weight={'f': .45, 't': .55}, random_state=SEED,  loss=\"modified_huber\"))\n",
    "            ])\n",
    "\n",
    "X = cafe_clean['joined_tokens']\n",
    "y = cafe_clean['t/f']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=SEED)\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test, target_names=cafe_clean['t/f'].unique()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=10) \n",
    "plot_confusion_matrix(final_model, X_test, y_test, ax=ax, normalize='true')\n",
    "plt.grid(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample = pd.read_csv('./data/reddit_sample_clean500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_sample[reddit_sample['joined_tokens'].isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_X = reddit_sample['joined_tokens']\n",
    "reddit_y = reddit_sample['t/f']\n",
    "\n",
    "y_pred = final_model.predict(reddit_X)\n",
    "acc_score = accuracy_score(reddit_y, y_pred)\n",
    "\n",
    "print(classification_report(y_pred=y_pred, y_true=reddit_y, target_names=reddit_sample['t/f'].unique()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=10) \n",
    "plot_confusion_matrix(final_model, reddit_X, reddit_y, ax=ax, normalize='true')\n",
    "plt.grid(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(final_model, open('./models/final_model.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test_sample = \"I love being in engaging conversation something that doesnt bore me something \\\n",
    "meaningful and makes both of you feel good. But sometimes I get called intense is this just me? \\\n",
    "Like I could say well sometimes you bore me. Seems like Im the minority so its \\\n",
    "really nice when I meet someone my vibe as there arent many\"\n",
    "\n",
    "print(final_model.predict([f_test_sample])[0], '--', f\"{round(max(final_model.predict_proba([f_test_sample])[0])*100, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_sample = \"Nature provides exceptions to every rule. But this rule is applied to itself paradoxically. \\\n",
    "(So there must be some rules without any exception :)\"\n",
    "\n",
    "print(final_model.predict([t_test_sample])[0], '--', f\"{round(max(final_model.predict_proba([t_test_sample])[0])*100, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(cafe_clean, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = train_set[train_set['t/f'] == 't']['joined_tokens']\n",
    "X_train_f = train_set[train_set['t/f'] == 'f']['joined_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_t = TfidfVectorizer(ngram_range=(1,2), min_df=50)\n",
    "\n",
    "train_tfidf_t = tfidf_t.fit_transform(X_train_t)\n",
    "feature_array_t = np.array(tfidf_t.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_f = TfidfVectorizer(ngram_range=(1,2), min_df=50)\n",
    "\n",
    "train_tfidf_f = tfidf_f.fit_transform(X_train_f)\n",
    "feature_array_f = np.array(tfidf_f.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_t = pd.DataFrame(train_tfidf_t.toarray(), columns=tfidf_t.get_feature_names())\n",
    "tfidf_df_f = pd.DataFrame(train_tfidf_f.toarray(), columns=tfidf_f.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_tfidf_t = []\n",
    "for column in tfidf_df_t.columns:\n",
    "    highest_tfidf_t.append((column, tfidf_df_t[column].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_tfidf_f = []\n",
    "for column in tfidf_df_f.columns:\n",
    "    highest_tfidf_f.append((column, tfidf_df_f[column].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKey(item):\n",
    "    return item[1]\n",
    "\n",
    "high_tfidf_t = sorted(highest_tfidf_t, key=getKey, reverse=True)\n",
    "high_tfidf_f = sorted(highest_tfidf_f, key=getKey, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('type likely', 0.8296687237727707),\n",
       " ('congrats', 0.8185622995048053),\n",
       " ('da', 0.7918117921110335),\n",
       " ('sir', 0.7700726251450812),\n",
       " ('grateful', 0.7564942644168047)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_tfidf_f[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('youtube', 0.8391669744279399),\n",
       " ('type likely', 0.7989372362119583),\n",
       " ('ha', 0.7974630459780876),\n",
       " ('blushed', 0.7874873975450024),\n",
       " ('metal', 0.7769961861565363)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_tfidf_t[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6505x10715 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2908737 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=50)\n",
    "\n",
    "X = cafe_clean['joined_tokens']\n",
    "y = cafe_clean['t/f']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=SEED)\n",
    "\n",
    "tfidf.fit_transform(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n",
      "['im' 'great' 'would great' 'oh im' 'year']\n",
      "-------------------------\n",
      "living interesting time le boring stability part world last decade hope im wrong soon lot action im traveling instead buying thing moment im rising budget im going hit road girlfriend longer period take time year save pile love thievery corporation annoying extrovert childhood im completely opposite people life get along trust like recent version much hehehe maybe try online dating case worked perfect okcupid com wrote honestly description found another well found another side glad know im one kind world hehehe thanks usually im hearing something like mean sad weird im reading im telling mind clear tooth stick long distance girlfriend philippine killed oh cute lollipop hehehe lol demisexual couldnt said better cat fantastic creature decided reduce consumption sold car replaced bike got rid lot stuff always prepare meal etc lifestyle probably lead loneliness kind great movie jagten imdb another make plan good mood day come would actually rather die see someone constantly seeking social interaction validation see blab nothing see get bored conversation becomes poor girl left brain right brain left brain percentage linear dominant characteristic verbal logical sequential reality based great show everytime public toilet american history wild generation war goal retire early live little log cabin near wilderness live saving grow im saving salary every month hope achieve goal im hitler dont want kill anyone child policy country highest population growth rate would great start utopia never happen billion would think might wrong especially part many people around future people lifetime increasing even younger people get two child per woman still maybe one problem spend year life realised make happy way life must choose well year le like hell point im listening voice hearth thanks response really like kid dont anything choice child well still great stepdad think near future planet harsh place cell phone year old player year old car year old etc thing still working great dont need new one nowadays industry make crap last short crazy today dealed situation received call old friend hadnt talked year recently thinking would great talk creepy unfortunately ill huge party weekend im sorry seem uninterested oh im listenin oh im indifferent truly aint got really great thread dont much happy might become le happier even depressed strong emotion lead emotional hangover balance key situation sharing personal born wrong century unfortunately word sometimes call yes man common youth nowadays much think im avoiding nightclub cleary unhealthy much noise club music piece crap overcrowded place energy quickly drained etc alone lithuania yes indeed really hate small talk chit chat mean strongly disagree opinion people made really angry face say shit pant didnt expected rage im calm person matter much need help prefer ask help possible way dont like much attention like running im time week first time completed km distance last week yay well im muscular soon gonna change little bit impressive cant say im nothing im thinking probably time many toughts floating without effort every session love moment focus important\n"
     ]
    }
   ],
   "source": [
    "rint = np.random.randint(0, len(X_test))\n",
    "feature_array = np.array(tfidf.get_feature_names())\n",
    "response = tfidf.transform([X_test[rint]])\n",
    "tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n",
    "\n",
    "n = 5\n",
    "top_n = feature_array[tfidf_sorting][:n]\n",
    "print(y_test[rint])\n",
    "print(top_n)\n",
    "print('-'*25)\n",
    "print(X_test[rint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf, open('./models/final_tfidf.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Inspect Thinking vs Feeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_fdist = FreqDist(cafe_clean[\"post_tokens\"][cafe_clean[\"t/f\"] == 't'].explode())\n",
    "f_fdist = FreqDist(cafe_clean[\"post_tokens\"][cafe_clean[\"t/f\"] == 'f'].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten = list(zip(*t_fdist.most_common(10)))\n",
    "tokens = top_ten[0]\n",
    "counts = top_ten[1]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.rc('xtick', labelsize=18) \n",
    "plt.rc('ytick', labelsize=18) \n",
    "plt.bar(tokens, counts)\n",
    "plt.title('Top 10 THINKING Class Word Occurences');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten = list(zip(*f_fdist.most_common(10)))\n",
    "tokens = top_ten[0]\n",
    "counts = top_ten[1]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.rc('xtick', labelsize=18) \n",
    "plt.rc('ytick', labelsize=18) \n",
    "plt.bar(tokens, counts)\n",
    "plt.title('Top 10 FEELING Class Word Occurences');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = cafe_clean[cafe_clean[\"t/f\"] == 't']\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "tf_cv = cv.fit(t_df['joined_tokens'])\n",
    "tf_cv = cv.transform(t_df['joined_tokens'])\n",
    "\n",
    "count_values = tf_cv.toarray().sum(axis=0)\n",
    "\n",
    "vocab = cv.vocabulary_\n",
    "\n",
    "df_ngram = pd.DataFrame(sorted([(count_values[i], k) for k, i in vocab.items()], reverse=True) \\\n",
    "                        ).rename(columns={0: 'frequency', 1:'bigram'})\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.bar(df_ngram.bigram.iloc[:10], df_ngram.frequency.iloc[:10]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = cafe_clean[cafe_clean[\"t/f\"] == 'f']\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "tf_cv = cv.fit(t_df['joined_tokens'])\n",
    "tf_cv = cv.transform(t_df['joined_tokens'])\n",
    "\n",
    "count_values = tf_cv.toarray().sum(axis=0)\n",
    "\n",
    "vocab = cv.vocabulary_\n",
    "\n",
    "df_ngram = pd.DataFrame(sorted([(count_values[i], k) for k, i in vocab.items()], reverse=True) \\\n",
    "                        ).rename(columns={0: 'frequency', 1:'bigram'})\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.bar(df_ngram.bigram.iloc[:10], df_ngram.frequency.iloc[:10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Random Sample Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = list(cafe_clean['type'].values)\n",
    "regxx='(intp)|(intj)|(entp)|(entj)|(infj)|(infp)|(enfj)|(enfp)|(istj)|(isfj)|(estj)|(esfj)|(istp)|(isfp)|(estp)|(esfp)|(intp)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv(\"./data/mbti_full_pull.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['author_flair_text'] = reddit_df['author_flair_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['t/f'] = reddit_df['author_flair_text'].map(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['t/f'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the random undersampler\n",
    "rus = RandomUnderSampler({'t':1000, 'f':1000}) \n",
    "\n",
    "# resampling training set X & y\n",
    "X_res, y_res = rus.fit_resample(X, y)\n",
    "\n",
    "y_pred = final_model.predict(X_res)\n",
    "acc_score = accuracy_score(y_res, y_pred)\n",
    "\n",
    "print(classification_report(y_pred=y_pred, y_true=y_res, target_names=reddit_df['t/f'].unique()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=10) \n",
    "plot_confusion_matrix(final_model, X_test, y_test, ax=ax, normalize='true')\n",
    "plt.grid(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking vs. Feeling\n",
    "\n",
    "**Thinking (T)**\n",
    "When I make a decision, I like to find the basic truth or principle to be applied, regardless of the specific situation involved. I like to analyze pros and cons, and then be consistent and logical in deciding. I try to be impersonal, so I won't let my personal wishes--or other people's wishes--influence me.\n",
    "\n",
    "The following statements generally apply to me:\n",
    "\n",
    "- I enjoy technical and scientific fields where logic is important.\n",
    "- I notice inconsistencies.\n",
    "- I look for logical explanations or solutions to most everything.\n",
    "- I make decisions with my head and want to be fair.\n",
    "- I believe telling the truth is more important than being tactful.\n",
    "- Sometimes I miss or don't value the \"people\" part of a situation.\n",
    "- I can be seen as too task-oriented, uncaring, or indifferent.\n",
    "\n",
    "**Feeling (F)**\n",
    "I believe I can make the best decisions by weighing what people care about and the points-of-view of persons involved in a situation. I am concerned with values and what is the best for the people involved. I like to do whatever will establish or maintain harmony. In my relationships, I appear caring, warm, and tactful.\n",
    "\n",
    "The following statements generally apply to me:\n",
    "\n",
    "- I have a people or communications orientation.\n",
    "- I am concerned with harmony and nervous when it is missing.\n",
    "- I look for what is important to others and express concern for others.\n",
    "- I make decisions with my heart and want to be compassionate.\n",
    "- I believe being tactful is more important than telling the \"cold\" truth.\n",
    "- Sometimes I miss seeing or communicating the \"hard truth\" of situations.\n",
    "- I am sometimes experienced by others as too idealistic, mushy, or indirect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 500 Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500 = pd.read_csv('./data/MBTI 500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500['clean_posts'] = df_500['posts'].apply(replace_mbti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500['type'] = df_500['type'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500 = df_500[df_500['clean_posts'].apply(lambda x: len(x.split())) > 450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = df_500['clean_posts'].apply(lambda x: len(x.split())).sum()\n",
    "print(f'The pre-cleaned tokens tally up to {total_words} total words')\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "ave_post = df_500['clean_posts'].apply(lambda x: len(x.split())).mean()\n",
    "print(f'Each feature in pre-cleaned has on average {round(ave_post)} words')\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "med_post = df_500['clean_posts'].apply(lambda x: len(x.split())).median()\n",
    "print(f'Each feature in pre-cleaned  has a median of {round(med_post)} words')\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "min_post = df_500['clean_posts'].apply(lambda x: len(x.split())).min()\n",
    "print(f'The minimum post in pre-cleaned  is {round(min_post)} words')\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "max_post = df_500['clean_posts'].apply(lambda x: len(x.split())).max()\n",
    "print(f'The minimum post in pre-cleaned  is {round(max_post)} words')\n",
    "\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.hist(df_500['clean_posts'].apply(lambda x: len(x.split())), label='pre-cleaned', alpha=.5, bins=100)\n",
    "plt.axvline(ave_post, color='k', linestyle='dashed', linewidth=3, label='pre-cleaned mean')\n",
    "plt.legend()\n",
    "plt.title('Distribution of Post Length \\n Clean vs Pre-Cleaned');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Binary Targets\n",
    "df_500['i/e'] = df_500['type'].map(lambda x: x[0])\n",
    "df_500['n/s'] = df_500['type'].map(lambda x: x[1])\n",
    "df_500['t/f'] = df_500['type'].map(lambda x: x[2])\n",
    "df_500['p/j'] = df_500['type'].map(lambda x: x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_500['i/e'].value_counts(normalize=True))\n",
    "print(df_500['n/s'].value_counts(normalize=True))\n",
    "print(df_500['t/f'].value_counts(normalize=True))\n",
    "print(df_500['p/j'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "\n",
    "baseline_models = {'SGDClassifier': SGDClassifier(class_weight='balanced', random_state=SEED),\n",
    "\n",
    "                #   'LogisticRegression': LogisticRegression(max_iter=1000, solver='saga'),\n",
    "\n",
    "                  'NaiveBayes': MultinomialNB()\n",
    "               \n",
    "                #   'DecisionTree': DecisionTreeClassifier(random_state=SEED),\n",
    "               \n",
    "                #   'RandomForest': RandomForestClassifier(random_state=SEED)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in baseline_models:\n",
    "    print(baseline_models[clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_table = pd.DataFrame(columns=['Model', 'Target', 'CVScore', 'TestAcc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_targets = cafe_clean.columns[-4:]\n",
    "bin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.run(df_500, 'clean_posts', bin_targets, baseline_models, models_table, tfidf=True, SEED=234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.run(df_500, 'clean_posts', bin_targets, baseline_models, models_table, tfidf=False, SEED=234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.run_usampled(df_500, 'clean_posts', bin_targets, baseline_models, models_table, tfidf=False, SEED=234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.run_usampled(df_500, 'clean_posts', bin_targets, baseline_models, models_table, tfidf=True, SEED=234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_500['i/e'].value_counts(normalize=False))\n",
    "print(df_500['n/s'].value_counts(normalize=False))\n",
    "print(df_500['t/f'].value_counts(normalize=False))\n",
    "print(df_500['p/j'].value_counts(normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.run_usampled(df_500, 'clean_posts', ['type'], baseline_models, models_table, tfidf=True, SEED=234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500[df_500.clean_posts.str.match(regxx) == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = pd.read_pickle('./pickle/finalmodeling_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.token_joined.str.match(regxx).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df['token_joined'] = final_df['token_joined'].apply(replace_mbti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = final_df[final_df.token_joined.str.match(regxx) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = final_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm.run(final_df, 'token_joined', bin_targets, baseline_models, models_table, tfidf=False, SEED=234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm.run(final_df, 'token_joined', bin_targets, baseline_models, models_table, tfidf=True, SEED=234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(final_df['i/e'].value_counts(normalize=True))\n",
    "# print(final_df['n/s'].value_counts(normalize=True))\n",
    "# print(final_df['t/f'].value_counts(normalize=True))\n",
    "# print(final_df['p/j'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rint = np.random.randint(0, len(final_df))\n",
    "\n",
    "# print(rint)\n",
    "# print(final_df['type'].iloc[rint])\n",
    "# final_df['token_joined'].iloc[rint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Methods\n",
    "\n",
    "## Results\n",
    "\n",
    "## Applications of the Predictive Model\n",
    "\n",
    "There are numerous applications for using this personality predictive model:\n",
    "\n",
    "- Customer Segmentation\n",
    "- Digital Advertising \n",
    "\n",
    "\n",
    "## Online Demo\n",
    "\n",
    "## Recommednations\n",
    "\n",
    "## Next Steps"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "033c84df5fb4c613acf884834f63930b25da6784759ce0fb831a430fcd673895"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('learn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
