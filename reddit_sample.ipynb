{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from nlp import preprocess\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv(\"./data/mbti_full_pull.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['author_flair_text'] = reddit_df['author_flair_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['author_flair_text'] = reddit_df['author_flair_text'].apply(lambda x: x if len(x) <= 4 else 'drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df[reddit_df['author_flair_text'] != 'drop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['t/f'] = reddit_df['author_flair_text'].map(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t    851722\n",
       "f    231912\n",
       "Name: t/f, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['t/f'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['body'] = reddit_df['body'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df[reddit_df['body'].apply(lambda x: len(x) > 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sample = reddit_df[reddit_df['t/f'] == 't'].sample(500, replace=False, random_state=22222)\n",
    "f_sample = reddit_df[reddit_df['t/f'] == 'f'].sample(500, replace=False, random_state=22222)\n",
    "\n",
    "reddit_sample = pd.concat([t_sample, f_sample], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f    500\n",
       "t    500\n",
       "Name: t/f, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_sample['t/f'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bringing in the prepreoccess class from mbti.py\n",
    "# This class provides functions to clean and tokenize our text data\n",
    "prepro = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the target names in the actual posts\n",
    "reddit_sample['clean_posts'] = reddit_sample['body'].apply(lambda x: prepro.replace_mbti(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The posts contain multiple posts seperated by 3 pipes '|||' w/ no spaces between. \n",
    "# This function will remove pipes and replace with a space.\n",
    "reddit_sample['clean_posts'] = reddit_sample['clean_posts'].apply(lambda x: prepro.pipe_remove(x))\n",
    "\n",
    "# This funciton will remove URLs in the posts\n",
    "reddit_sample['clean_posts'] = reddit_sample['clean_posts'].apply(lambda x: prepro.url_remove(x))\n",
    "\n",
    "# This function will remove punctuation (dependent on what is passed in). This has `/``, `_`, `:` \n",
    "reddit_sample['clean_posts'] = reddit_sample['clean_posts'].apply(lambda x: prepro.punc_remove(x))\n",
    "\n",
    "# Removes all characters that are not American Standard Code for Information Interchange\n",
    "reddit_sample['clean_posts'] = reddit_sample['clean_posts'].apply(lambda x: prepro.remove_symbols(x))\n",
    "\n",
    "# Fixes all spelling errors\n",
    "# reddit_sample['clean_posts'] = reddit_sample['clean_posts'].apply(lambda x: prepro.spelling(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create a column of cleaned words that have been tokenized.\n",
    "pattern = r\"(?u)\\b\\w\\w+\\b\" # words with more than 2 letters\n",
    "tokenizer = RegexpTokenizer(pattern) # instantiate tokenizer\n",
    "reddit_sample['post_tokens'] = reddit_sample['clean_posts'].apply(tokenizer.tokenize) # Tokenize to new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing any remaining numeric digits\n",
    "reddit_sample['post_tokens'] = reddit_sample['post_tokens'].apply(lambda x: prepro.remove_dig_token(x))\n",
    "\n",
    "# Removing stopwords\n",
    "reddit_sample['post_tokens'] = reddit_sample['post_tokens'].apply(lambda x: prepro.remove_stopwords(x))\n",
    "\n",
    "# Lemmatizing the words with POS tagging\n",
    "reddit_sample['post_tokens'] = reddit_sample['post_tokens'].apply(lambda x: prepro.lemmend_pos(x, pos=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the tokens together into one long string\n",
    "reddit_sample['joined_tokens'] = reddit_sample['post_tokens'].apply(lambda x: prepro.join_tokens(x)) # Creating new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_sample['joined_tokens'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>t/f</th>\n",
       "      <th>clean_posts</th>\n",
       "      <th>post_tokens</th>\n",
       "      <th>joined_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16807</th>\n",
       "      <td>intj</td>\n",
       "      <td>Start reading [Mr. Money Mustache.](http://www...</td>\n",
       "      <td>intj</td>\n",
       "      <td>t</td>\n",
       "      <td>Start reading [Mr. Money Mustache.]( and subsc...</td>\n",
       "      <td>[Start, reading, Mr, Money, Mustache, subscrib...</td>\n",
       "      <td>Start reading Mr Money Mustache subscribe frug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15826</th>\n",
       "      <td>intj</td>\n",
       "      <td>&amp;gt; I honestly think it just upsets people th...</td>\n",
       "      <td>intj</td>\n",
       "      <td>t</td>\n",
       "      <td>&amp;gt; I honestly think it just upsets people th...</td>\n",
       "      <td>[gt, honestly, think, upset, people, work, eas...</td>\n",
       "      <td>gt honestly think upset people work easier say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11588</th>\n",
       "      <td>istp</td>\n",
       "      <td>My interpretation, without having read any exp...</td>\n",
       "      <td>mbti</td>\n",
       "      <td>t</td>\n",
       "      <td>My interpretation, without having read any exp...</td>\n",
       "      <td>[My, interpretation, without, read, explanatio...</td>\n",
       "      <td>My interpretation without read explanation The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12667</th>\n",
       "      <td>intj</td>\n",
       "      <td>As someone who is making the current transitio...</td>\n",
       "      <td>intj</td>\n",
       "      <td>t</td>\n",
       "      <td>As someone who is making the current transitio...</td>\n",
       "      <td>[As, someone, making, current, transition, car...</td>\n",
       "      <td>As someone making current transition career tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>intp</td>\n",
       "      <td>&amp;gt; cruel even though I know it isn't intende...</td>\n",
       "      <td>INTP</td>\n",
       "      <td>t</td>\n",
       "      <td>&amp;gt; cruel even though I know it isnt intended...</td>\n",
       "      <td>[gt, cruel, even, though, know, isnt, intended...</td>\n",
       "      <td>gt cruel even though know isnt intended way Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3290</th>\n",
       "      <td>enfj</td>\n",
       "      <td>thank you @sugoruyo for your in-depth reply. i...</td>\n",
       "      <td>INTP</td>\n",
       "      <td>f</td>\n",
       "      <td>thank you @sugoruyo for your in-depth reply. i...</td>\n",
       "      <td>[thank, sugoruyo, depth, reply, comforting, in...</td>\n",
       "      <td>thank sugoruyo depth reply comforting informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16677</th>\n",
       "      <td>infj</td>\n",
       "      <td>Oh, but it's actually great advice! I have to ...</td>\n",
       "      <td>infj</td>\n",
       "      <td>f</td>\n",
       "      <td>Oh, but its actually great advice! I have to r...</td>\n",
       "      <td>[Oh, actually, great, advice, remind, time, ti...</td>\n",
       "      <td>Oh actually great advice remind time time Ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>enfp</td>\n",
       "      <td>Just give it some time until your \"attachment\"...</td>\n",
       "      <td>ENFP</td>\n",
       "      <td>f</td>\n",
       "      <td>Just give it some time until your \"attachment\"...</td>\n",
       "      <td>[Just, give, time, attachment, people, past, f...</td>\n",
       "      <td>Just give time attachment people past fade Try...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>infj</td>\n",
       "      <td>Holy narrow view batman. This whole video hone...</td>\n",
       "      <td>infj</td>\n",
       "      <td>f</td>\n",
       "      <td>Holy narrow view batman. This whole video hone...</td>\n",
       "      <td>[Holy, narrow, view, batman, This, whole, vide...</td>\n",
       "      <td>Holy narrow view batman This whole video hones...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>infj</td>\n",
       "      <td>&amp;gt; It doesn't help that secretly any interac...</td>\n",
       "      <td>infj</td>\n",
       "      <td>f</td>\n",
       "      <td>&amp;gt; It doesnt help that secretly any interact...</td>\n",
       "      <td>[gt, It, doesnt, help, secretly, interaction, ...</td>\n",
       "      <td>gt It doesnt help secretly interaction anyone ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      author_flair_text  ...                                      joined_tokens\n",
       "16807              intj  ...  Start reading Mr Money Mustache subscribe frug...\n",
       "15826              intj  ...  gt honestly think upset people work easier say...\n",
       "11588              istp  ...  My interpretation without read explanation The...\n",
       "12667              intj  ...  As someone making current transition career tr...\n",
       "2814               intp  ...  gt cruel even though know isnt intended way Cr...\n",
       "...                 ...  ...                                                ...\n",
       "3290               enfj  ...  thank sugoruyo depth reply comforting informat...\n",
       "16677              infj  ...  Oh actually great advice remind time time Ther...\n",
       "499                enfp  ...  Just give time attachment people past fade Try...\n",
       "676                infj  ...  Holy narrow view batman This whole video hones...\n",
       "3340               infj  ...  gt It doesnt help secretly interaction anyone ...\n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/reddit_sample.csv'\n",
    "reddit_sample.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "033c84df5fb4c613acf884834f63930b25da6784759ce0fb831a430fcd673895"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
