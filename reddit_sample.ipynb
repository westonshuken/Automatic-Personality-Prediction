{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from mbti import preprocess\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv(\"./data/mbti_full_pull.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['author_flair_text'] = reddit_df['author_flair_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['author_flair_text'] = reddit_df['author_flair_text'].apply(lambda x: x if len(x) <= 4 else 'drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df[reddit_df['author_flair_text'] != 'drop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['t/f'] = reddit_df['author_flair_text'].map(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t    851722\n",
       "f    231912\n",
       "Name: t/f, dtype: int64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['t/f'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['body'] = reddit_df['body'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df[reddit_df['body'].apply(lambda x: len(x) > 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sample = reddit_df[reddit_df['t/f'] == 't'].sample(500, replace=False, random_state=22222)\n",
    "f_sample = reddit_df[reddit_df['t/f'] == 'f'].sample(500, replace=False, random_state=22222)\n",
    "\n",
    "reddit_sample = pd.concat([t_sample, f_sample], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f    500\n",
       "t    500\n",
       "Name: t/f, dtype: int64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_sample['t/f'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bringing in the prepreoccess class from mbti.py\n",
    "# This class provides functions to clean and tokenize our text data\n",
    "prepro = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the target names in the actual posts\n",
    "reddit_sample['clean_posts'] = reddit_sample['body'].apply(lambda x: prepro.replace_mbti(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The posts contain multiple posts seperated by 3 pipes '|||' w/ no spaces between. \n",
    "# This function will remove pipes and replace with a space.\n",
    "reddit_sample['clean_posts'] = reddit_sample['clean_posts'].apply(lambda x: prepro.pipe_remove(x))\n",
    "\n",
    "# This funciton will remove URLs in the posts\n",
    "reddit_sample['clean_posts'] = reddit_sample['clean_posts'].apply(lambda x: prepro.url_remove(x))\n",
    "\n",
    "# This function will remove punctuation (dependent on what is passed in). This has `/``, `_`, `:` \n",
    "reddit_sample['clean_posts'] = reddit_sample['clean_posts'].apply(lambda x: prepro.punc_remove(x))\n",
    "\n",
    "# Removes all characters that are not American Standard Code for Information Interchange\n",
    "reddit_sample['clean_posts'] = reddit_sample['clean_posts'].apply(lambda x: prepro.remove_symbols(x))\n",
    "\n",
    "# Fixes all spelling errors\n",
    "# reddit_sample['clean_posts'] = reddit_sample['clean_posts'].apply(lambda x: prepro.spelling(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create a column of cleaned words that have been tokenized.\n",
    "pattern = r\"(?u)\\b\\w\\w+\\b\" # words with more than 2 letters\n",
    "tokenizer = RegexpTokenizer(pattern) # instantiate tokenizer\n",
    "reddit_sample['post_tokens'] = reddit_sample['clean_posts'].apply(tokenizer.tokenize) # Tokenize to new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing any remaining numeric digits\n",
    "reddit_sample['post_tokens'] = reddit_sample['post_tokens'].apply(lambda x: prepro.remove_dig_token(x))\n",
    "\n",
    "# Removing stopwords\n",
    "reddit_sample['post_tokens'] = reddit_sample['post_tokens'].apply(lambda x: prepro.remove_stopwords(x))\n",
    "\n",
    "# Lemmatizing the words with POS tagging\n",
    "reddit_sample['post_tokens'] = reddit_sample['post_tokens'].apply(lambda x: prepro.lemmend_pos(x, pos=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the tokens together into one long string\n",
    "reddit_sample['joined_tokens'] = reddit_sample['post_tokens'].apply(lambda x: prepro.join_tokens(x)) # Creating new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_sample['joined_tokens'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/reddit_sample_clean500.csv'\n",
    "reddit_sample.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "033c84df5fb4c613acf884834f63930b25da6784759ce0fb831a430fcd673895"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
